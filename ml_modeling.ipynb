{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('model_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Year</th>\n",
       "      <th>Player</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tm</th>\n",
       "      <th>G</th>\n",
       "      <th>MP</th>\n",
       "      <th>PER</th>\n",
       "      <th>TS%</th>\n",
       "      <th>...</th>\n",
       "      <th>DBPM</th>\n",
       "      <th>3P%</th>\n",
       "      <th>2P%</th>\n",
       "      <th>FT%</th>\n",
       "      <th>TmNetRtg</th>\n",
       "      <th>Next Rtg</th>\n",
       "      <th>Next WS</th>\n",
       "      <th>Veteran Value</th>\n",
       "      <th>VV Class</th>\n",
       "      <th>Player Level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1997</td>\n",
       "      <td>Vinny Del</td>\n",
       "      <td>SG</td>\n",
       "      <td>30.0</td>\n",
       "      <td>SAS</td>\n",
       "      <td>72.0</td>\n",
       "      <td>2243.0</td>\n",
       "      <td>14.4</td>\n",
       "      <td>0.529</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.3</td>\n",
       "      <td>0.314</td>\n",
       "      <td>0.501</td>\n",
       "      <td>0.868</td>\n",
       "      <td>-8.8</td>\n",
       "      <td>4.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>2903.225806</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1997</td>\n",
       "      <td>Avery Johnson</td>\n",
       "      <td>PG</td>\n",
       "      <td>31.0</td>\n",
       "      <td>SAS</td>\n",
       "      <td>76.0</td>\n",
       "      <td>2472.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.517</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.231</td>\n",
       "      <td>0.487</td>\n",
       "      <td>0.690</td>\n",
       "      <td>-8.8</td>\n",
       "      <td>4.4</td>\n",
       "      <td>6.7</td>\n",
       "      <td>14558.823529</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>1997</td>\n",
       "      <td>Charles Barkley</td>\n",
       "      <td>PF</td>\n",
       "      <td>33.0</td>\n",
       "      <td>HOU</td>\n",
       "      <td>53.0</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.581</td>\n",
       "      <td>...</td>\n",
       "      <td>2.8</td>\n",
       "      <td>0.283</td>\n",
       "      <td>0.569</td>\n",
       "      <td>0.694</td>\n",
       "      <td>4.7</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>8.6</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>1997</td>\n",
       "      <td>Clyde Drexler</td>\n",
       "      <td>SG</td>\n",
       "      <td>34.0</td>\n",
       "      <td>HOU</td>\n",
       "      <td>62.0</td>\n",
       "      <td>2271.0</td>\n",
       "      <td>19.9</td>\n",
       "      <td>0.548</td>\n",
       "      <td>...</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.355</td>\n",
       "      <td>0.493</td>\n",
       "      <td>0.750</td>\n",
       "      <td>4.7</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>6.8</td>\n",
       "      <td>7.423895</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>1997</td>\n",
       "      <td>Mario Elie</td>\n",
       "      <td>SF</td>\n",
       "      <td>33.0</td>\n",
       "      <td>HOU</td>\n",
       "      <td>78.0</td>\n",
       "      <td>2687.0</td>\n",
       "      <td>14.3</td>\n",
       "      <td>0.662</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.572</td>\n",
       "      <td>0.896</td>\n",
       "      <td>4.7</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>3.8</td>\n",
       "      <td>2.079266</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>811</th>\n",
       "      <td>1409</td>\n",
       "      <td>2020</td>\n",
       "      <td>Dwight Howard</td>\n",
       "      <td>C</td>\n",
       "      <td>34.0</td>\n",
       "      <td>LAL</td>\n",
       "      <td>69.0</td>\n",
       "      <td>1306.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>0.696</td>\n",
       "      <td>...</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.732</td>\n",
       "      <td>0.514</td>\n",
       "      <td>5.6</td>\n",
       "      <td>5.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.068878</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>812</th>\n",
       "      <td>1410</td>\n",
       "      <td>2020</td>\n",
       "      <td>LeBron James</td>\n",
       "      <td>PG</td>\n",
       "      <td>35.0</td>\n",
       "      <td>LAL</td>\n",
       "      <td>67.0</td>\n",
       "      <td>2316.0</td>\n",
       "      <td>25.5</td>\n",
       "      <td>0.577</td>\n",
       "      <td>...</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.348</td>\n",
       "      <td>0.564</td>\n",
       "      <td>0.693</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.9</td>\n",
       "      <td>5.6</td>\n",
       "      <td>1.125000</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813</th>\n",
       "      <td>1412</td>\n",
       "      <td>2020</td>\n",
       "      <td>Justin Holiday</td>\n",
       "      <td>SG</td>\n",
       "      <td>30.0</td>\n",
       "      <td>IND</td>\n",
       "      <td>73.0</td>\n",
       "      <td>1826.0</td>\n",
       "      <td>12.1</td>\n",
       "      <td>0.585</td>\n",
       "      <td>...</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.405</td>\n",
       "      <td>0.477</td>\n",
       "      <td>0.791</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3.1</td>\n",
       "      <td>3.617225</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814</th>\n",
       "      <td>1416</td>\n",
       "      <td>2020</td>\n",
       "      <td>Paul Millsap</td>\n",
       "      <td>PF</td>\n",
       "      <td>34.0</td>\n",
       "      <td>DEN</td>\n",
       "      <td>51.0</td>\n",
       "      <td>1240.0</td>\n",
       "      <td>16.9</td>\n",
       "      <td>0.591</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.435</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.816</td>\n",
       "      <td>2.2</td>\n",
       "      <td>4.8</td>\n",
       "      <td>3.2</td>\n",
       "      <td>-1012.987013</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>815</th>\n",
       "      <td>1422</td>\n",
       "      <td>2020</td>\n",
       "      <td>Thaddeus Young</td>\n",
       "      <td>PF</td>\n",
       "      <td>31.0</td>\n",
       "      <td>CHI</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1591.0</td>\n",
       "      <td>13.3</td>\n",
       "      <td>0.521</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.356</td>\n",
       "      <td>0.501</td>\n",
       "      <td>0.583</td>\n",
       "      <td>-3.1</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>10865.874363</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>816 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  Year           Player Pos   Age   Tm     G      MP   PER  \\\n",
       "0             1  1997        Vinny Del  SG  30.0  SAS  72.0  2243.0  14.4   \n",
       "1             3  1997    Avery Johnson  PG  31.0  SAS  76.0  2472.0  15.0   \n",
       "2             7  1997  Charles Barkley  PF  33.0  HOU  53.0  2009.0  23.0   \n",
       "3             8  1997    Clyde Drexler  SG  34.0  HOU  62.0  2271.0  19.9   \n",
       "4             9  1997       Mario Elie  SF  33.0  HOU  78.0  2687.0  14.3   \n",
       "..          ...   ...              ...  ..   ...  ...   ...     ...   ...   \n",
       "811        1409  2020    Dwight Howard   C  34.0  LAL  69.0  1306.0  18.7   \n",
       "812        1410  2020     LeBron James  PG  35.0  LAL  67.0  2316.0  25.5   \n",
       "813        1412  2020   Justin Holiday  SG  30.0  IND  73.0  1826.0  12.1   \n",
       "814        1416  2020     Paul Millsap  PF  34.0  DEN  51.0  1240.0  16.9   \n",
       "815        1422  2020   Thaddeus Young  PF  31.0  CHI  64.0  1591.0  13.3   \n",
       "\n",
       "       TS%  ...  DBPM    3P%    2P%    FT%  TmNetRtg  Next Rtg  Next WS  \\\n",
       "0    0.529  ...  -2.3  0.314  0.501  0.868      -8.8       4.4      3.7   \n",
       "1    0.517  ...  -3.0  0.231  0.487  0.690      -8.8       4.4      6.7   \n",
       "2    0.581  ...   2.8  0.283  0.569  0.694       4.7      -0.9      8.6   \n",
       "3    0.548  ...   1.7  0.355  0.493  0.750       4.7      -0.9      6.8   \n",
       "4    0.662  ...   0.1  0.420  0.572  0.896       4.7      -0.9      3.8   \n",
       "..     ...  ...   ...    ...    ...    ...       ...       ...      ...   \n",
       "811  0.696  ...   1.2  0.600  0.732  0.514       5.6       5.5      4.0   \n",
       "812  0.577  ...   1.8  0.348  0.564  0.693       5.6       2.9      5.6   \n",
       "813  0.585  ...   1.8  0.405  0.477  0.791       1.9       0.1      3.1   \n",
       "814  0.591  ...  -0.2  0.435  0.500  0.816       2.2       4.8      3.2   \n",
       "815  0.521  ...   0.2  0.356  0.501  0.583      -3.1      -1.1      5.1   \n",
       "\n",
       "     Veteran Value  VV Class  Player Level  \n",
       "0      2903.225806         0             1  \n",
       "1     14558.823529         0             1  \n",
       "2        14.000000         0             2  \n",
       "3         7.423895         0             2  \n",
       "4         2.079266         0             1  \n",
       "..             ...       ...           ...  \n",
       "811       0.068878         0             0  \n",
       "812       1.125000         0             3  \n",
       "813       3.617225         0             1  \n",
       "814   -1012.987013         1             2  \n",
       "815   10865.874363         0             1  \n",
       "\n",
       "[816 rows x 34 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Year</th>\n",
       "      <th>Player</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tm</th>\n",
       "      <th>G</th>\n",
       "      <th>MP</th>\n",
       "      <th>PER</th>\n",
       "      <th>TS%</th>\n",
       "      <th>...</th>\n",
       "      <th>DBPM</th>\n",
       "      <th>3P%</th>\n",
       "      <th>2P%</th>\n",
       "      <th>FT%</th>\n",
       "      <th>TmNetRtg</th>\n",
       "      <th>Next Rtg</th>\n",
       "      <th>Next WS</th>\n",
       "      <th>Veteran Value</th>\n",
       "      <th>VV Class</th>\n",
       "      <th>Player Level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>608</td>\n",
       "      <td>2006</td>\n",
       "      <td>Ray Allen</td>\n",
       "      <td>SG</td>\n",
       "      <td>30.0</td>\n",
       "      <td>SEA</td>\n",
       "      <td>78.0</td>\n",
       "      <td>3022.0</td>\n",
       "      <td>22.2</td>\n",
       "      <td>0.590</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.1</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.486</td>\n",
       "      <td>0.903</td>\n",
       "      <td>-3.3</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>6.3</td>\n",
       "      <td>-306.220096</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>659</td>\n",
       "      <td>2007</td>\n",
       "      <td>Ray Allen</td>\n",
       "      <td>SG</td>\n",
       "      <td>31.0</td>\n",
       "      <td>SEA</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2219.0</td>\n",
       "      <td>21.6</td>\n",
       "      <td>0.564</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.1</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.479</td>\n",
       "      <td>0.903</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>10.8</td>\n",
       "      <td>9.7</td>\n",
       "      <td>24825.396825</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>721</td>\n",
       "      <td>2008</td>\n",
       "      <td>Ray Allen</td>\n",
       "      <td>SG</td>\n",
       "      <td>32.0</td>\n",
       "      <td>BOS</td>\n",
       "      <td>73.0</td>\n",
       "      <td>2624.0</td>\n",
       "      <td>16.4</td>\n",
       "      <td>0.584</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.398</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.907</td>\n",
       "      <td>10.8</td>\n",
       "      <td>7.9</td>\n",
       "      <td>11.1</td>\n",
       "      <td>26.851852</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>775</td>\n",
       "      <td>2009</td>\n",
       "      <td>Ray Allen</td>\n",
       "      <td>SG</td>\n",
       "      <td>33.0</td>\n",
       "      <td>BOS</td>\n",
       "      <td>79.0</td>\n",
       "      <td>2876.0</td>\n",
       "      <td>17.3</td>\n",
       "      <td>0.624</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>0.409</td>\n",
       "      <td>0.542</td>\n",
       "      <td>0.952</td>\n",
       "      <td>7.9</td>\n",
       "      <td>3.7</td>\n",
       "      <td>7.9</td>\n",
       "      <td>1.844146</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>826</td>\n",
       "      <td>2010</td>\n",
       "      <td>Ray Allen</td>\n",
       "      <td>SG</td>\n",
       "      <td>34.0</td>\n",
       "      <td>BOS</td>\n",
       "      <td>80.0</td>\n",
       "      <td>2819.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>0.601</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.913</td>\n",
       "      <td>3.7</td>\n",
       "      <td>5.7</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1436.879918</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>883</td>\n",
       "      <td>2011</td>\n",
       "      <td>Ray Allen</td>\n",
       "      <td>SG</td>\n",
       "      <td>35.0</td>\n",
       "      <td>BOS</td>\n",
       "      <td>80.0</td>\n",
       "      <td>2890.0</td>\n",
       "      <td>16.4</td>\n",
       "      <td>0.615</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.520</td>\n",
       "      <td>0.881</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.6</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.026150</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>939</td>\n",
       "      <td>2012</td>\n",
       "      <td>Ray Allen</td>\n",
       "      <td>SG</td>\n",
       "      <td>36.0</td>\n",
       "      <td>BOS</td>\n",
       "      <td>46.0</td>\n",
       "      <td>1565.0</td>\n",
       "      <td>14.8</td>\n",
       "      <td>0.607</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.453</td>\n",
       "      <td>0.463</td>\n",
       "      <td>0.915</td>\n",
       "      <td>2.6</td>\n",
       "      <td>8.2</td>\n",
       "      <td>5.4</td>\n",
       "      <td>3207.855974</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>987</td>\n",
       "      <td>2013</td>\n",
       "      <td>Ray Allen</td>\n",
       "      <td>SG</td>\n",
       "      <td>37.0</td>\n",
       "      <td>MIA</td>\n",
       "      <td>79.0</td>\n",
       "      <td>2035.0</td>\n",
       "      <td>14.7</td>\n",
       "      <td>0.599</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>0.419</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.886</td>\n",
       "      <td>8.2</td>\n",
       "      <td>5.1</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1.570356</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  Year     Player Pos   Age   Tm     G      MP   PER    TS%  \\\n",
       "344         608  2006  Ray Allen  SG  30.0  SEA  78.0  3022.0  22.2  0.590   \n",
       "375         659  2007  Ray Allen  SG  31.0  SEA  55.0  2219.0  21.6  0.564   \n",
       "407         721  2008  Ray Allen  SG  32.0  BOS  73.0  2624.0  16.4  0.584   \n",
       "440         775  2009  Ray Allen  SG  33.0  BOS  79.0  2876.0  17.3  0.624   \n",
       "472         826  2010  Ray Allen  SG  34.0  BOS  80.0  2819.0  15.2  0.601   \n",
       "506         883  2011  Ray Allen  SG  35.0  BOS  80.0  2890.0  16.4  0.615   \n",
       "540         939  2012  Ray Allen  SG  36.0  BOS  46.0  1565.0  14.8  0.607   \n",
       "573         987  2013  Ray Allen  SG  37.0  MIA  79.0  2035.0  14.7  0.599   \n",
       "\n",
       "     ...  DBPM    3P%    2P%    FT%  TmNetRtg  Next Rtg  Next WS  \\\n",
       "344  ...  -3.1  0.412  0.486  0.903      -3.3      -3.0      6.3   \n",
       "375  ...  -2.1  0.372  0.479  0.903      -3.0      10.8      9.7   \n",
       "407  ...  -0.1  0.398  0.485  0.907      10.8       7.9     11.1   \n",
       "440  ...  -0.3  0.409  0.542  0.952       7.9       3.7      7.9   \n",
       "472  ...  -0.7  0.363  0.556  0.913       3.7       5.7     10.0   \n",
       "506  ...   0.4  0.444  0.520  0.881       5.7       2.6      4.7   \n",
       "540  ...   0.3  0.453  0.463  0.915       2.6       8.2      5.4   \n",
       "573  ...  -1.5  0.419  0.480  0.886       8.2       5.1      4.1   \n",
       "\n",
       "     Veteran Value  VV Class  Player Level  \n",
       "344    -306.220096         1             2  \n",
       "375   24825.396825         0             2  \n",
       "407      26.851852         0             2  \n",
       "440       1.844146         0             2  \n",
       "472    1436.879918         0             2  \n",
       "506       1.026150         0             2  \n",
       "540    3207.855974         0             2  \n",
       "573       1.570356         0             2  \n",
       "\n",
       "[8 rows x 34 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['Player']=='Ray Allen']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Year', 'Player', 'Pos', 'Age', 'Tm', 'G', 'MP', 'PER',\n",
       "       'TS%', '3PAr', 'FTr', 'ORB%', 'DRB%', 'AST%', 'STL%', 'BLK%', 'TOV%',\n",
       "       'USG%', 'OWS', 'DWS', 'WS', 'WS/48', 'OBPM', 'DBPM', '3P%', '2P%',\n",
       "       'FT%', 'TmNetRtg', 'Next Rtg', 'Next WS', 'Veteran Value', 'VV Class',\n",
       "       'Player Level'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Frequency'>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['Veteran Value'].plot.hist(grid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-74952.88448704804 83684.74167012605\n"
     ]
    }
   ],
   "source": [
    "# calculate summary statistics\n",
    "data = df['Veteran Value']\n",
    "data_mean, data_std = np.mean(data), np.std(data)\n",
    "# identify outliers\n",
    "cut_off = data_std * 3\n",
    "lower, upper = data_mean - cut_off, data_mean + cut_off\n",
    "print(lower, upper)\n",
    "lower, upper = -30000,30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17,\n",
       " 22,\n",
       " 25,\n",
       " 100,\n",
       " 110,\n",
       " 116,\n",
       " 141,\n",
       " 142,\n",
       " 150,\n",
       " 153,\n",
       " 207,\n",
       " 238,\n",
       " 259,\n",
       " 269,\n",
       " 272,\n",
       " 280,\n",
       " 295,\n",
       " 425,\n",
       " 466,\n",
       " 467,\n",
       " 468,\n",
       " 561,\n",
       " 564,\n",
       " 588,\n",
       " 602,\n",
       " 618,\n",
       " 625,\n",
       " 655,\n",
       " 663,\n",
       " 664,\n",
       " 681,\n",
       " 684,\n",
       " 686,\n",
       " 695,\n",
       " 697,\n",
       " 728,\n",
       " 732,\n",
       " 785,\n",
       " 799,\n",
       " 803]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outliers = [x[0] for x in enumerate(df['Veteran Value']) if x[1] < lower or x[1] > upper]\n",
    "outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(index=outliers,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Frequency'>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['Veteran Value'].plot.hist(grid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2.   4.  15.  30. 595.  60.  21.  23.  13.  13.] [-25787.54578755 -20461.87762854 -15136.20946954  -9810.54131054\n",
      "  -4484.87315154    840.79500746   6166.46316646  11492.13132546\n",
      "  16817.79948447  22143.46764347  27469.13580247]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n, bins, patches = plt.hist(df['Veteran Value'])\n",
    "print(n,bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-732.0261437908493 0.9964010920824024 2.7948529411764707 18.63425925925926 403.9479039479037\n",
      "-2410.3629746212873\n"
     ]
    }
   ],
   "source": [
    "print(df['Veteran Value'].quantile(0.16),df['Veteran Value'].quantile(0.33),df['Veteran Value'].quantile(0.49),\n",
    "      df['Veteran Value'].quantile(0.66),df['Veteran Value'].quantile(0.82))\n",
    "print(df['Veteran Value'].quantile(0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df['Veteran Value']\n",
    "\n",
    "def getClass(vv):\n",
    "    if vv < -732:\n",
    "        return 0\n",
    "    elif -732 <= vv < 0:\n",
    "        return 1\n",
    "    elif 0 <= vv < 20:\n",
    "        return 2\n",
    "    elif 20 <= vv < 400:\n",
    "        return 3\n",
    "    elif 400 < vv:\n",
    "        return 4\n",
    "\n",
    "    \n",
    "df['VV Class'] = [getClass(vv) for vv in df['Veteran Value']]\n",
    "target_class = df['VV Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count      776.000000\n",
       "mean       701.920421\n",
       "std       5762.341195\n",
       "min     -25787.545788\n",
       "25%          0.039286\n",
       "50%          3.069227\n",
       "75%         86.598774\n",
       "max      27469.135802\n",
       "Name: Veteran Value, dtype: float64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Veteran Value'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Unnamed: 0','Age','Year','Player','Pos','Tm','Next WS','WS','Next Rtg','TmNetRtg','Veteran Value', 'VV Class'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['G', 'MP', 'PER', 'TS%', '3PAr', 'FTr', 'ORB%', 'DRB%', 'AST%', 'STL%',\n",
       "       'BLK%', 'TOV%', 'USG%', 'OWS', 'DWS', 'WS/48', 'OBPM', 'DBPM', '3P%',\n",
       "       '2P%', 'FT%', 'Player Level'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try scaling values\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(scaled_data, target_class.values, random_state=42,test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4935483870967742\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "clf = LogisticRegression(tol=1e-4, max_iter=1000, random_state=0).fit(xtrain, ytrain)\n",
    "clf.predict(xtest)\n",
    "clf.predict_proba(xtest)\n",
    "scores = cross_val_score(clf, xtest, ytest, cv=5)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6332282815850263"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.roc_auc_score(ytest,clf.predict_proba(xtest),multi_class='ovr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=15)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "n_neighbors=15\n",
    "neigh = KNeighborsClassifier(n_neighbors)\n",
    "neigh.fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(neigh, xtest, ytest, cv=5)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5774719586545947"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.roc_auc_score(ytest,neigh.predict_proba(xtest),multi_class='ovr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.11356361, 0.07944814, 0.58561728, 0.11690895, 0.10446202],\n",
       "       [0.16945175, 0.06772763, 0.44102257, 0.15908143, 0.16271662],\n",
       "       [0.08812813, 0.02544672, 0.21639779, 0.22950344, 0.44052393],\n",
       "       [0.25349628, 0.1284866 , 0.25685019, 0.19788188, 0.16328505],\n",
       "       [0.21108302, 0.11386335, 0.34188028, 0.19595182, 0.13722153],\n",
       "       [0.14019758, 0.0565391 , 0.22894587, 0.28277542, 0.29154203],\n",
       "       [0.10536566, 0.1270744 , 0.61420052, 0.07857573, 0.07478369],\n",
       "       [0.15313563, 0.05759803, 0.28557344, 0.14775967, 0.35593322],\n",
       "       [0.09112997, 0.04408583, 0.18987538, 0.37420417, 0.30070464],\n",
       "       [0.13143604, 0.08959017, 0.58035636, 0.09161494, 0.10700249],\n",
       "       [0.11803799, 0.10504407, 0.44581229, 0.19523216, 0.1358735 ],\n",
       "       [0.12598106, 0.07291374, 0.61024702, 0.10555435, 0.08530384],\n",
       "       [0.23348411, 0.12458317, 0.44378016, 0.09686738, 0.10128517],\n",
       "       [0.15130533, 0.03089027, 0.18442878, 0.17394195, 0.45943366],\n",
       "       [0.27828104, 0.09440826, 0.41949666, 0.11740892, 0.09040512],\n",
       "       [0.29686815, 0.06855238, 0.36819198, 0.11856725, 0.14782024],\n",
       "       [0.11712331, 0.09967477, 0.5790994 , 0.12938709, 0.07471543],\n",
       "       [0.16948592, 0.10456048, 0.39040519, 0.2355889 , 0.0999595 ],\n",
       "       [0.13773215, 0.06567818, 0.43833888, 0.1296316 , 0.22861918],\n",
       "       [0.08307879, 0.03853398, 0.28717108, 0.30132043, 0.28989572],\n",
       "       [0.08394577, 0.04012093, 0.30365554, 0.24638428, 0.32589348],\n",
       "       [0.18540222, 0.05606775, 0.29732082, 0.21414125, 0.24706795],\n",
       "       [0.11778522, 0.03675427, 0.18613232, 0.35439801, 0.30493018],\n",
       "       [0.11525827, 0.07133546, 0.28426998, 0.28813114, 0.24100515],\n",
       "       [0.11365311, 0.0520094 , 0.58925271, 0.16692547, 0.07815931],\n",
       "       [0.15650469, 0.0721498 , 0.41488444, 0.10864836, 0.24781271],\n",
       "       [0.09392586, 0.02621862, 0.32882607, 0.23302127, 0.31800818],\n",
       "       [0.10441777, 0.08140638, 0.59196352, 0.13662525, 0.08558709],\n",
       "       [0.07915069, 0.11147728, 0.64818951, 0.10666913, 0.05451339],\n",
       "       [0.17133565, 0.04352918, 0.2655694 , 0.20387674, 0.31568903],\n",
       "       [0.0954225 , 0.07370561, 0.20922125, 0.25280795, 0.3688427 ],\n",
       "       [0.1323722 , 0.09753831, 0.49222527, 0.13803795, 0.13982627],\n",
       "       [0.2495797 , 0.03540367, 0.27847139, 0.08433538, 0.35220985],\n",
       "       [0.18065307, 0.096794  , 0.36665981, 0.17265215, 0.18324097],\n",
       "       [0.0896546 , 0.0281467 , 0.16004988, 0.28184959, 0.44029923],\n",
       "       [0.08124367, 0.07622343, 0.64927033, 0.10754816, 0.08571442],\n",
       "       [0.09959301, 0.02913122, 0.36914215, 0.29307233, 0.20906128],\n",
       "       [0.33410629, 0.08866845, 0.3100918 , 0.12873987, 0.13839358],\n",
       "       [0.10271699, 0.03305067, 0.22046298, 0.23667888, 0.40709047],\n",
       "       [0.15427895, 0.09417935, 0.52234689, 0.12508209, 0.10411272],\n",
       "       [0.13995066, 0.09190011, 0.32321632, 0.2863755 , 0.15855741],\n",
       "       [0.17396535, 0.11936941, 0.3184741 , 0.24266124, 0.1455299 ],\n",
       "       [0.28818278, 0.06680788, 0.44327974, 0.11200735, 0.08972225],\n",
       "       [0.15544884, 0.11730841, 0.46336901, 0.11655049, 0.14732325],\n",
       "       [0.21154924, 0.11751126, 0.32825293, 0.2142446 , 0.12844197],\n",
       "       [0.08138508, 0.08733288, 0.57928367, 0.15535254, 0.09664583],\n",
       "       [0.08862284, 0.04537521, 0.16275685, 0.33236498, 0.37088012],\n",
       "       [0.11387183, 0.04867098, 0.68050159, 0.11729787, 0.03965772],\n",
       "       [0.22266593, 0.09993698, 0.40044191, 0.18474991, 0.09220527],\n",
       "       [0.08376355, 0.04838775, 0.27187582, 0.24536896, 0.35060392],\n",
       "       [0.14682173, 0.06819229, 0.4513093 , 0.10495846, 0.22871822],\n",
       "       [0.09657686, 0.06584517, 0.32412973, 0.32783953, 0.18560871],\n",
       "       [0.18355693, 0.120256  , 0.41641175, 0.16245543, 0.1173199 ],\n",
       "       [0.27525059, 0.04799864, 0.26109256, 0.07430073, 0.34135748],\n",
       "       [0.05725298, 0.04569024, 0.15179887, 0.32544847, 0.41980944],\n",
       "       [0.16677964, 0.05488627, 0.41010399, 0.20847506, 0.15975503],\n",
       "       [0.25307419, 0.05887249, 0.31623795, 0.08845136, 0.28336401],\n",
       "       [0.39871193, 0.10339527, 0.23664594, 0.1020386 , 0.15920826],\n",
       "       [0.1216179 , 0.04225076, 0.24099608, 0.30036819, 0.29476707],\n",
       "       [0.17257054, 0.06344024, 0.4451718 , 0.10981226, 0.20900515],\n",
       "       [0.06905648, 0.03196762, 0.17128213, 0.33165949, 0.39603428],\n",
       "       [0.16984765, 0.03859491, 0.35193739, 0.09126046, 0.34835958],\n",
       "       [0.0692303 , 0.0646111 , 0.75206109, 0.07462838, 0.03946912],\n",
       "       [0.10324373, 0.06768335, 0.65590355, 0.11392429, 0.05924508],\n",
       "       [0.3199825 , 0.10895794, 0.2936964 , 0.12364962, 0.15371354],\n",
       "       [0.14367286, 0.10858195, 0.49919595, 0.15901663, 0.0895326 ],\n",
       "       [0.14989616, 0.11318474, 0.36162296, 0.26914368, 0.10615246],\n",
       "       [0.08377538, 0.06121273, 0.69074708, 0.10676645, 0.05749836],\n",
       "       [0.18726265, 0.10232506, 0.52207868, 0.11162395, 0.07670966],\n",
       "       [0.31271453, 0.09283437, 0.36665932, 0.11940989, 0.10838188],\n",
       "       [0.21278824, 0.08030398, 0.38050671, 0.11912753, 0.20727354],\n",
       "       [0.23726945, 0.09768501, 0.43386188, 0.1227198 , 0.10846386],\n",
       "       [0.14510955, 0.05894555, 0.42655068, 0.13236383, 0.23703039],\n",
       "       [0.16970612, 0.10008631, 0.49670653, 0.09664838, 0.13685267],\n",
       "       [0.2778311 , 0.09791554, 0.36384065, 0.14317874, 0.11723397],\n",
       "       [0.11447959, 0.11163623, 0.5220818 , 0.14520535, 0.10659703],\n",
       "       [0.15752161, 0.0484099 , 0.32477784, 0.12653793, 0.34275272],\n",
       "       [0.12796287, 0.0997071 , 0.59856016, 0.08864678, 0.08512309],\n",
       "       [0.15862046, 0.0479136 , 0.50756782, 0.12142227, 0.16447585],\n",
       "       [0.31872208, 0.07113898, 0.3279862 , 0.13381388, 0.14833886],\n",
       "       [0.09118163, 0.08999043, 0.54483198, 0.20392486, 0.0700711 ],\n",
       "       [0.07304499, 0.05258328, 0.18753125, 0.37311771, 0.31372277],\n",
       "       [0.17027385, 0.07611045, 0.50814984, 0.12775979, 0.11770608],\n",
       "       [0.1139157 , 0.03663819, 0.14157154, 0.23450056, 0.47337401],\n",
       "       [0.11796341, 0.08798146, 0.53120143, 0.16903899, 0.09381471],\n",
       "       [0.11098501, 0.04023899, 0.34342517, 0.24154045, 0.26381039],\n",
       "       [0.11873441, 0.01149206, 0.16161628, 0.21356275, 0.4945945 ],\n",
       "       [0.11863491, 0.06804977, 0.34629211, 0.14703208, 0.31999113],\n",
       "       [0.22586668, 0.08111703, 0.24925339, 0.26455196, 0.17921094],\n",
       "       [0.35593171, 0.08632539, 0.3551215 , 0.08413688, 0.11848453],\n",
       "       [0.30265144, 0.13973529, 0.28687903, 0.14111701, 0.12961723],\n",
       "       [0.20101781, 0.13899656, 0.39470295, 0.19691751, 0.06836516],\n",
       "       [0.18849086, 0.12761584, 0.49783382, 0.08981391, 0.09624557],\n",
       "       [0.22668874, 0.05782967, 0.50389622, 0.11720997, 0.0943754 ],\n",
       "       [0.10511602, 0.08060489, 0.39394084, 0.21859344, 0.20174481],\n",
       "       [0.14153835, 0.05611061, 0.35832425, 0.15282529, 0.2912015 ],\n",
       "       [0.12117919, 0.02863235, 0.33042635, 0.25947051, 0.2602916 ],\n",
       "       [0.13733071, 0.03703841, 0.40803244, 0.09599498, 0.32160346],\n",
       "       [0.13505407, 0.07538055, 0.55647047, 0.17209024, 0.06100467],\n",
       "       [0.20080581, 0.0502656 , 0.29195114, 0.14895477, 0.30802268],\n",
       "       [0.15092754, 0.08038804, 0.56271471, 0.14357803, 0.06239168],\n",
       "       [0.08650507, 0.0367974 , 0.154326  , 0.14044825, 0.58192328],\n",
       "       [0.11137185, 0.06657248, 0.59083091, 0.15600482, 0.07521994],\n",
       "       [0.25799155, 0.09102783, 0.42073199, 0.12472104, 0.10552758],\n",
       "       [0.09153291, 0.08059949, 0.6762737 , 0.0953743 , 0.05621959],\n",
       "       [0.0831361 , 0.06415393, 0.23726812, 0.35096723, 0.26447462],\n",
       "       [0.0764323 , 0.06798432, 0.41193046, 0.25609447, 0.18755844],\n",
       "       [0.11867058, 0.0466702 , 0.62771189, 0.11344552, 0.09350181],\n",
       "       [0.11039722, 0.1426273 , 0.48164271, 0.13910214, 0.12623063],\n",
       "       [0.19990041, 0.11094459, 0.48534558, 0.12000487, 0.08380456],\n",
       "       [0.1749502 , 0.03969347, 0.52043055, 0.13682399, 0.1281018 ],\n",
       "       [0.13875615, 0.083133  , 0.557071  , 0.12438465, 0.09665521],\n",
       "       [0.14289856, 0.08895804, 0.39628209, 0.22600393, 0.14585738],\n",
       "       [0.18700506, 0.11402579, 0.40140422, 0.17339723, 0.1241677 ],\n",
       "       [0.07468109, 0.07237664, 0.27525531, 0.33887496, 0.238812  ],\n",
       "       [0.08844394, 0.07399736, 0.35308701, 0.2739308 , 0.21054088],\n",
       "       [0.17222977, 0.07429478, 0.46216518, 0.16657582, 0.12473445],\n",
       "       [0.11315509, 0.06099319, 0.3680587 , 0.19640888, 0.26138414],\n",
       "       [0.14851529, 0.07182159, 0.39771541, 0.21943964, 0.16250807],\n",
       "       [0.25575559, 0.07278236, 0.42430496, 0.12579642, 0.12136067],\n",
       "       [0.10364464, 0.11754029, 0.63343268, 0.0817142 , 0.06366819],\n",
       "       [0.14953269, 0.07719059, 0.59646465, 0.09937346, 0.07743861],\n",
       "       [0.13231249, 0.08642539, 0.4030443 , 0.18994628, 0.18827153],\n",
       "       [0.12842027, 0.04905094, 0.44635896, 0.1473968 , 0.22877302],\n",
       "       [0.10146213, 0.0400568 , 0.42859982, 0.12145274, 0.30842851],\n",
       "       [0.29376298, 0.07718445, 0.34022745, 0.15436438, 0.13446075],\n",
       "       [0.14878685, 0.07217939, 0.43139215, 0.17113199, 0.17650962],\n",
       "       [0.22351354, 0.05348123, 0.51934511, 0.10635731, 0.09730281],\n",
       "       [0.11639878, 0.03338269, 0.26060129, 0.21066935, 0.37894789],\n",
       "       [0.15043872, 0.08283152, 0.5794367 , 0.0790604 , 0.10823266],\n",
       "       [0.08280207, 0.0309849 , 0.11124166, 0.10551604, 0.66945533],\n",
       "       [0.11309791, 0.0546276 , 0.31498894, 0.294111  , 0.22317455],\n",
       "       [0.11326429, 0.19862821, 0.5178113 , 0.1032297 , 0.0670665 ],\n",
       "       [0.08026002, 0.03178089, 0.15757869, 0.22394717, 0.50643323],\n",
       "       [0.145312  , 0.08478077, 0.50514762, 0.17428192, 0.09047768],\n",
       "       [0.21351124, 0.08541812, 0.34272546, 0.1866512 , 0.17169398],\n",
       "       [0.09012207, 0.03369246, 0.23024735, 0.22740989, 0.41852823],\n",
       "       [0.15655557, 0.0623024 , 0.26404799, 0.28855133, 0.22854272],\n",
       "       [0.11346501, 0.04343376, 0.18053289, 0.24534552, 0.41722282],\n",
       "       [0.09914654, 0.0409953 , 0.26776506, 0.31888656, 0.27320654],\n",
       "       [0.10982394, 0.05662114, 0.43022195, 0.14887847, 0.2544545 ],\n",
       "       [0.10185645, 0.05660087, 0.56063076, 0.10982783, 0.1710841 ],\n",
       "       [0.0880352 , 0.03455333, 0.08220063, 0.09372702, 0.70148382],\n",
       "       [0.09396829, 0.07555135, 0.67005293, 0.09630804, 0.06411939],\n",
       "       [0.17392035, 0.08440846, 0.41760865, 0.15750555, 0.16655698],\n",
       "       [0.13123829, 0.09415682, 0.60330871, 0.11743363, 0.05386255],\n",
       "       [0.13747603, 0.03052195, 0.20481673, 0.3040492 , 0.32313609],\n",
       "       [0.30209478, 0.05730934, 0.36244242, 0.14556736, 0.1325861 ],\n",
       "       [0.23078751, 0.04587903, 0.37472318, 0.14368799, 0.20492229],\n",
       "       [0.19277092, 0.10960334, 0.3672536 , 0.2408183 , 0.08955384],\n",
       "       [0.35115844, 0.09666079, 0.27917708, 0.13932117, 0.13368252],\n",
       "       [0.116691  , 0.07770271, 0.5960538 , 0.12758725, 0.08196524],\n",
       "       [0.121142  , 0.04715282, 0.22924931, 0.28150536, 0.32095051],\n",
       "       [0.29685591, 0.0643431 , 0.42023051, 0.08598483, 0.13258564],\n",
       "       [0.18919048, 0.12748021, 0.42820236, 0.13649387, 0.11863309],\n",
       "       [0.16982361, 0.04442464, 0.2857139 , 0.25598483, 0.24405302]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=250, max_depth=50, min_samples_leaf=10, min_samples_split = 10, max_features=10)\n",
    "rf.fit(xtrain, ytrain)\n",
    "rf.predict_proba(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4870967741935484\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(rf, xtest, ytest, cv=5)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6280369748033003"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.roc_auc_score(ytest,rf.predict_proba(xtest),multi_class='ovr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosted Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=10,\n",
       "                                                         min_samples_split=64),\n",
       "                   learning_rate=0.5, n_estimators=225)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "abc = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(min_samples_split=2**6, max_depth=10), \n",
    "                         n_estimators=225, learning_rate=0.5)\n",
    "abc.fit(xtrain, ytrain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4806451612903226\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(abc, xtest, ytest, cv=5)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5550724436685286"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.roc_auc_score(ytest,abc.predict_proba(xtest),multi_class='ovr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.57616926\n",
      "Iteration 2, loss = 1.47811263\n",
      "Iteration 3, loss = 1.45994764\n",
      "Iteration 4, loss = 1.44801503\n",
      "Iteration 5, loss = 1.43871961\n",
      "Iteration 6, loss = 1.42571202\n",
      "Iteration 7, loss = 1.41332040\n",
      "Iteration 8, loss = 1.40788119\n",
      "Iteration 9, loss = 1.39721201\n",
      "Iteration 10, loss = 1.39194451\n",
      "Iteration 11, loss = 1.38186205\n",
      "Iteration 12, loss = 1.37706881\n",
      "Iteration 13, loss = 1.37049992\n",
      "Iteration 14, loss = 1.36429286\n",
      "Iteration 15, loss = 1.36075018\n",
      "Iteration 16, loss = 1.35742583\n",
      "Iteration 17, loss = 1.35244096\n",
      "Iteration 18, loss = 1.34807396\n",
      "Iteration 19, loss = 1.33956155\n",
      "Iteration 20, loss = 1.33932475\n",
      "Iteration 21, loss = 1.33228098\n",
      "Iteration 22, loss = 1.32796595\n",
      "Iteration 23, loss = 1.32423514\n",
      "Iteration 24, loss = 1.31894743\n",
      "Iteration 25, loss = 1.31418584\n",
      "Iteration 26, loss = 1.31555214\n",
      "Iteration 27, loss = 1.31562683\n",
      "Iteration 28, loss = 1.30304627\n",
      "Iteration 29, loss = 1.30034037\n",
      "Iteration 30, loss = 1.30266910\n",
      "Iteration 31, loss = 1.29629995\n",
      "Iteration 32, loss = 1.28584671\n",
      "Iteration 33, loss = 1.28283791\n",
      "Iteration 34, loss = 1.27954820\n",
      "Iteration 35, loss = 1.27953993\n",
      "Iteration 36, loss = 1.28216055\n",
      "Iteration 37, loss = 1.27171590\n",
      "Iteration 38, loss = 1.26388908\n",
      "Iteration 39, loss = 1.26437378\n",
      "Iteration 40, loss = 1.26775625\n",
      "Iteration 41, loss = 1.25946369\n",
      "Iteration 42, loss = 1.25577231\n",
      "Iteration 43, loss = 1.25082760\n",
      "Iteration 44, loss = 1.24635267\n",
      "Iteration 45, loss = 1.24361258\n",
      "Iteration 46, loss = 1.23905617\n",
      "Iteration 47, loss = 1.23626962\n",
      "Iteration 48, loss = 1.23106464\n",
      "Iteration 49, loss = 1.24506366\n",
      "Iteration 50, loss = 1.24180053\n",
      "Iteration 51, loss = 1.22517563\n",
      "Iteration 52, loss = 1.22886512\n",
      "Iteration 53, loss = 1.21607539\n",
      "Iteration 54, loss = 1.21683269\n",
      "Iteration 55, loss = 1.21037640\n",
      "Iteration 56, loss = 1.21128832\n",
      "Iteration 57, loss = 1.20755341\n",
      "Iteration 58, loss = 1.20815188\n",
      "Iteration 59, loss = 1.20310870\n",
      "Iteration 60, loss = 1.19913583\n",
      "Iteration 61, loss = 1.19584942\n",
      "Iteration 62, loss = 1.18723712\n",
      "Iteration 63, loss = 1.18700344\n",
      "Iteration 64, loss = 1.18669028\n",
      "Iteration 65, loss = 1.18829255\n",
      "Iteration 66, loss = 1.20427572\n",
      "Iteration 67, loss = 1.17961680\n",
      "Iteration 68, loss = 1.16802594\n",
      "Iteration 69, loss = 1.16856114\n",
      "Iteration 70, loss = 1.17201018\n",
      "Iteration 71, loss = 1.16960526\n",
      "Iteration 72, loss = 1.15941280\n",
      "Iteration 73, loss = 1.15091831\n",
      "Iteration 74, loss = 1.16201534\n",
      "Iteration 75, loss = 1.15714400\n",
      "Iteration 76, loss = 1.17781772\n",
      "Iteration 77, loss = 1.17415206\n",
      "Iteration 78, loss = 1.15328551\n",
      "Iteration 79, loss = 1.13633885\n",
      "Iteration 80, loss = 1.15769892\n",
      "Iteration 81, loss = 1.13405236\n",
      "Iteration 82, loss = 1.12614128\n",
      "Iteration 83, loss = 1.12398753\n",
      "Iteration 84, loss = 1.12309910\n",
      "Iteration 85, loss = 1.13100909\n",
      "Iteration 86, loss = 1.11855649\n",
      "Iteration 87, loss = 1.10925465\n",
      "Iteration 88, loss = 1.10445685\n",
      "Iteration 89, loss = 1.10461479\n",
      "Iteration 90, loss = 1.09970905\n",
      "Iteration 91, loss = 1.09984245\n",
      "Iteration 92, loss = 1.09133363\n",
      "Iteration 93, loss = 1.09019431\n",
      "Iteration 94, loss = 1.08416414\n",
      "Iteration 95, loss = 1.07738019\n",
      "Iteration 96, loss = 1.07510495\n",
      "Iteration 97, loss = 1.07910715\n",
      "Iteration 98, loss = 1.08986210\n",
      "Iteration 99, loss = 1.07025986\n",
      "Iteration 100, loss = 1.06480399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(batch_size=256, hidden_layer_sizes=(1000, 100, 10), max_iter=100,\n",
       "              verbose=True)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlpc = MLPClassifier(hidden_layer_sizes=(1000,100,10),activation='relu',learning_rate_init=0.001,batch_size=256, verbose=True,max_iter=100)\n",
    "mlpc.fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:542: UserWarning: Got `batch_size` less than 1 or larger than sample size. It is going to be clipped\n",
      "  warnings.warn(\"Got `batch_size` less than 1 or larger than \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.77152837\n",
      "Iteration 2, loss = 1.66199349\n",
      "Iteration 3, loss = 1.60704710\n",
      "Iteration 4, loss = 1.56658202\n",
      "Iteration 5, loss = 1.52281180\n",
      "Iteration 6, loss = 1.48019459\n",
      "Iteration 7, loss = 1.44198691\n",
      "Iteration 8, loss = 1.40965787\n",
      "Iteration 9, loss = 1.38543699\n",
      "Iteration 10, loss = 1.37157543\n",
      "Iteration 11, loss = 1.36732670\n",
      "Iteration 12, loss = 1.36970823\n",
      "Iteration 13, loss = 1.37473869\n",
      "Iteration 14, loss = 1.37856025\n",
      "Iteration 15, loss = 1.37973544\n",
      "Iteration 16, loss = 1.37718005\n",
      "Iteration 17, loss = 1.36996938\n",
      "Iteration 18, loss = 1.36008092\n",
      "Iteration 19, loss = 1.35021059\n",
      "Iteration 20, loss = 1.34108191\n",
      "Iteration 21, loss = 1.33321801\n",
      "Iteration 22, loss = 1.32673877\n",
      "Iteration 23, loss = 1.32180357\n",
      "Iteration 24, loss = 1.31804644\n",
      "Iteration 25, loss = 1.31514096\n",
      "Iteration 26, loss = 1.31223050\n",
      "Iteration 27, loss = 1.30914560\n",
      "Iteration 28, loss = 1.30549497\n",
      "Iteration 29, loss = 1.30098794\n",
      "Iteration 30, loss = 1.29589034\n",
      "Iteration 31, loss = 1.29058707\n",
      "Iteration 32, loss = 1.28536493\n",
      "Iteration 33, loss = 1.28033302\n",
      "Iteration 34, loss = 1.27564202\n",
      "Iteration 35, loss = 1.27113389\n",
      "Iteration 36, loss = 1.26679260\n",
      "Iteration 37, loss = 1.26242670\n",
      "Iteration 38, loss = 1.25790655\n",
      "Iteration 39, loss = 1.25321258\n",
      "Iteration 40, loss = 1.24846819\n",
      "Iteration 41, loss = 1.24380913\n",
      "Iteration 42, loss = 1.23909853\n",
      "Iteration 43, loss = 1.23433295\n",
      "Iteration 44, loss = 1.22956964\n",
      "Iteration 45, loss = 1.22484852\n",
      "Iteration 46, loss = 1.22027853\n",
      "Iteration 47, loss = 1.21568956\n",
      "Iteration 48, loss = 1.21108639\n",
      "Iteration 49, loss = 1.20644468\n",
      "Iteration 50, loss = 1.20159578\n",
      "Iteration 51, loss = 1.19661618\n",
      "Iteration 52, loss = 1.19157463\n",
      "Iteration 53, loss = 1.18657834\n",
      "Iteration 54, loss = 1.18150501\n",
      "Iteration 55, loss = 1.17635058\n",
      "Iteration 56, loss = 1.17116208\n",
      "Iteration 57, loss = 1.16596514\n",
      "Iteration 58, loss = 1.16067052\n",
      "Iteration 59, loss = 1.15534111\n",
      "Iteration 60, loss = 1.14986365\n",
      "Iteration 61, loss = 1.14433370\n",
      "Iteration 62, loss = 1.13869791\n",
      "Iteration 63, loss = 1.13307116\n",
      "Iteration 64, loss = 1.12731499\n",
      "Iteration 65, loss = 1.12138713\n",
      "Iteration 66, loss = 1.11522770\n",
      "Iteration 67, loss = 1.10908535\n",
      "Iteration 68, loss = 1.10274335\n",
      "Iteration 69, loss = 1.09598376\n",
      "Iteration 70, loss = 1.08914844\n",
      "Iteration 71, loss = 1.08212549\n",
      "Iteration 72, loss = 1.07474780\n",
      "Iteration 73, loss = 1.06714613\n",
      "Iteration 74, loss = 1.05970497\n",
      "Iteration 75, loss = 1.05183833\n",
      "Iteration 76, loss = 1.04339325\n",
      "Iteration 77, loss = 1.03521289\n",
      "Iteration 78, loss = 1.02719586\n",
      "Iteration 79, loss = 1.01923073\n",
      "Iteration 80, loss = 1.01123903\n",
      "Iteration 81, loss = 1.00302498\n",
      "Iteration 82, loss = 0.99471137\n",
      "Iteration 83, loss = 0.98613539\n",
      "Iteration 84, loss = 0.97748797\n",
      "Iteration 85, loss = 0.96859595\n",
      "Iteration 86, loss = 0.95996740\n",
      "Iteration 87, loss = 0.95108947\n",
      "Iteration 88, loss = 0.94195337\n",
      "Iteration 89, loss = 0.93303545\n",
      "Iteration 90, loss = 0.92462724\n",
      "Iteration 91, loss = 0.91614868\n",
      "Iteration 92, loss = 0.90753713\n",
      "Iteration 93, loss = 0.89906748\n",
      "Iteration 94, loss = 0.89086404\n",
      "Iteration 95, loss = 0.88201574\n",
      "Iteration 96, loss = 0.87339031\n",
      "Iteration 97, loss = 0.86500431\n",
      "Iteration 98, loss = 0.85694768\n",
      "Iteration 99, loss = 0.84931938\n",
      "Iteration 100, loss = 0.84101644\n",
      "Iteration 1, loss = 1.50032704\n",
      "Iteration 2, loss = 1.47100740\n",
      "Iteration 3, loss = 1.45413773\n",
      "Iteration 4, loss = 1.44119976\n",
      "Iteration 5, loss = 1.43003410\n",
      "Iteration 6, loss = 1.41887345\n",
      "Iteration 7, loss = 1.41115677\n",
      "Iteration 8, loss = 1.40408496\n",
      "Iteration 9, loss = 1.39619983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:542: UserWarning: Got `batch_size` less than 1 or larger than sample size. It is going to be clipped\n",
      "  warnings.warn(\"Got `batch_size` less than 1 or larger than \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10, loss = 1.38784391\n",
      "Iteration 11, loss = 1.37995449\n",
      "Iteration 12, loss = 1.37194129\n",
      "Iteration 13, loss = 1.36386844\n",
      "Iteration 14, loss = 1.35577786\n",
      "Iteration 15, loss = 1.34760650\n",
      "Iteration 16, loss = 1.33937859\n",
      "Iteration 17, loss = 1.33171273\n",
      "Iteration 18, loss = 1.32438875\n",
      "Iteration 19, loss = 1.31748112\n",
      "Iteration 20, loss = 1.31051136\n",
      "Iteration 21, loss = 1.30365512\n",
      "Iteration 22, loss = 1.29680162\n",
      "Iteration 23, loss = 1.29007206\n",
      "Iteration 24, loss = 1.28295196\n",
      "Iteration 25, loss = 1.27567236\n",
      "Iteration 26, loss = 1.26806766\n",
      "Iteration 27, loss = 1.26029590\n",
      "Iteration 28, loss = 1.25224304\n",
      "Iteration 29, loss = 1.24382435\n",
      "Iteration 30, loss = 1.23540304\n",
      "Iteration 31, loss = 1.22704993\n",
      "Iteration 32, loss = 1.21859309\n",
      "Iteration 33, loss = 1.20998287\n",
      "Iteration 34, loss = 1.20147959\n",
      "Iteration 35, loss = 1.19302011\n",
      "Iteration 36, loss = 1.18474121\n",
      "Iteration 37, loss = 1.17653887\n",
      "Iteration 38, loss = 1.16837692\n",
      "Iteration 39, loss = 1.16017187\n",
      "Iteration 40, loss = 1.15204571\n",
      "Iteration 41, loss = 1.14393162\n",
      "Iteration 42, loss = 1.13602703\n",
      "Iteration 43, loss = 1.12809303\n",
      "Iteration 44, loss = 1.12031374\n",
      "Iteration 45, loss = 1.11257653\n",
      "Iteration 46, loss = 1.10512921\n",
      "Iteration 47, loss = 1.09817477\n",
      "Iteration 48, loss = 1.09265808\n",
      "Iteration 49, loss = 1.08930632\n",
      "Iteration 50, loss = 1.08302893\n",
      "Iteration 51, loss = 1.07246332\n",
      "Iteration 52, loss = 1.07063963\n",
      "Iteration 53, loss = 1.06449459\n",
      "Iteration 54, loss = 1.05657164\n",
      "Iteration 55, loss = 1.05443909\n",
      "Iteration 56, loss = 1.04538755\n",
      "Iteration 57, loss = 1.04235325\n",
      "Iteration 58, loss = 1.03615479\n",
      "Iteration 59, loss = 1.03001186\n",
      "Iteration 60, loss = 1.02635166\n",
      "Iteration 61, loss = 1.01872865\n",
      "Iteration 62, loss = 1.01463177\n",
      "Iteration 63, loss = 1.00881418\n",
      "Iteration 64, loss = 1.00246734\n",
      "Iteration 65, loss = 0.99825636\n",
      "Iteration 66, loss = 0.99135339\n",
      "Iteration 67, loss = 0.98608038\n",
      "Iteration 68, loss = 0.98091352\n",
      "Iteration 69, loss = 0.97413320\n",
      "Iteration 70, loss = 0.96904371\n",
      "Iteration 71, loss = 0.96355213\n",
      "Iteration 72, loss = 0.95665890\n",
      "Iteration 73, loss = 0.95151194\n",
      "Iteration 74, loss = 0.94613531\n",
      "Iteration 75, loss = 0.93885426\n",
      "Iteration 76, loss = 0.93308325\n",
      "Iteration 77, loss = 0.92817782\n",
      "Iteration 78, loss = 0.92110837\n",
      "Iteration 79, loss = 0.91422017\n",
      "Iteration 80, loss = 0.90874023\n",
      "Iteration 81, loss = 0.90351849\n",
      "Iteration 82, loss = 0.89758296\n",
      "Iteration 83, loss = 0.88986048\n",
      "Iteration 84, loss = 0.88275152\n",
      "Iteration 85, loss = 0.87670521\n",
      "Iteration 86, loss = 0.87049712\n",
      "Iteration 87, loss = 0.86430043\n",
      "Iteration 88, loss = 0.85786575\n",
      "Iteration 89, loss = 0.85130049\n",
      "Iteration 90, loss = 0.84422207\n",
      "Iteration 91, loss = 0.83714742\n",
      "Iteration 92, loss = 0.83052747\n",
      "Iteration 93, loss = 0.82344096\n",
      "Iteration 94, loss = 0.81661219\n",
      "Iteration 95, loss = 0.80968080\n",
      "Iteration 96, loss = 0.80293156\n",
      "Iteration 97, loss = 0.79720754\n",
      "Iteration 98, loss = 0.79365466\n",
      "Iteration 99, loss = 0.79183534\n",
      "Iteration 100, loss = 0.78668351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:542: UserWarning: Got `batch_size` less than 1 or larger than sample size. It is going to be clipped\n",
      "  warnings.warn(\"Got `batch_size` less than 1 or larger than \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.55202712\n",
      "Iteration 2, loss = 1.52524709\n",
      "Iteration 3, loss = 1.49800884\n",
      "Iteration 4, loss = 1.47536558\n",
      "Iteration 5, loss = 1.45741012\n",
      "Iteration 6, loss = 1.44434580\n",
      "Iteration 7, loss = 1.43294691\n",
      "Iteration 8, loss = 1.42298189\n",
      "Iteration 9, loss = 1.41385614\n",
      "Iteration 10, loss = 1.40564907\n",
      "Iteration 11, loss = 1.39763039\n",
      "Iteration 12, loss = 1.38917167\n",
      "Iteration 13, loss = 1.38067667\n",
      "Iteration 14, loss = 1.37253091\n",
      "Iteration 15, loss = 1.36479837\n",
      "Iteration 16, loss = 1.35760369\n",
      "Iteration 17, loss = 1.35082957\n",
      "Iteration 18, loss = 1.34471985\n",
      "Iteration 19, loss = 1.34009147\n",
      "Iteration 20, loss = 1.33598770\n",
      "Iteration 21, loss = 1.33132067\n",
      "Iteration 22, loss = 1.32582303\n",
      "Iteration 23, loss = 1.31979143\n",
      "Iteration 24, loss = 1.31378523\n",
      "Iteration 25, loss = 1.30696081\n",
      "Iteration 26, loss = 1.30085467\n",
      "Iteration 27, loss = 1.29488169\n",
      "Iteration 28, loss = 1.28841760\n",
      "Iteration 29, loss = 1.28200780\n",
      "Iteration 30, loss = 1.27592571\n",
      "Iteration 31, loss = 1.26993886\n",
      "Iteration 32, loss = 1.26419969\n",
      "Iteration 33, loss = 1.25830061\n",
      "Iteration 34, loss = 1.25220706\n",
      "Iteration 35, loss = 1.24608616\n",
      "Iteration 36, loss = 1.24030934\n",
      "Iteration 37, loss = 1.23423520\n",
      "Iteration 38, loss = 1.22757375\n",
      "Iteration 39, loss = 1.22071771\n",
      "Iteration 40, loss = 1.21445900\n",
      "Iteration 41, loss = 1.20805856\n",
      "Iteration 42, loss = 1.20099336\n",
      "Iteration 43, loss = 1.19418104\n",
      "Iteration 44, loss = 1.18710309\n",
      "Iteration 45, loss = 1.17933558\n",
      "Iteration 46, loss = 1.17192268\n",
      "Iteration 47, loss = 1.16421259\n",
      "Iteration 48, loss = 1.15600810\n",
      "Iteration 49, loss = 1.14830130\n",
      "Iteration 50, loss = 1.14067875\n",
      "Iteration 51, loss = 1.13270624\n",
      "Iteration 52, loss = 1.12471845\n",
      "Iteration 53, loss = 1.11669726\n",
      "Iteration 54, loss = 1.10875523\n",
      "Iteration 55, loss = 1.10125192\n",
      "Iteration 56, loss = 1.09353722\n",
      "Iteration 57, loss = 1.08599365\n",
      "Iteration 58, loss = 1.07875182\n",
      "Iteration 59, loss = 1.07118945\n",
      "Iteration 60, loss = 1.06397357\n",
      "Iteration 61, loss = 1.05658732\n",
      "Iteration 62, loss = 1.04950653\n",
      "Iteration 63, loss = 1.04252914\n",
      "Iteration 64, loss = 1.03532057\n",
      "Iteration 65, loss = 1.02840041\n",
      "Iteration 66, loss = 1.02161799\n",
      "Iteration 67, loss = 1.01520315\n",
      "Iteration 68, loss = 1.00936715\n",
      "Iteration 69, loss = 1.00322876\n",
      "Iteration 70, loss = 0.99704871\n",
      "Iteration 71, loss = 0.99116558\n",
      "Iteration 72, loss = 0.98469854\n",
      "Iteration 73, loss = 0.97964236\n",
      "Iteration 74, loss = 0.97327211\n",
      "Iteration 75, loss = 0.96746348\n",
      "Iteration 76, loss = 0.96232673\n",
      "Iteration 77, loss = 0.95558234\n",
      "Iteration 78, loss = 0.95048248\n",
      "Iteration 79, loss = 0.94544750\n",
      "Iteration 80, loss = 0.93901379\n",
      "Iteration 81, loss = 0.93363270\n",
      "Iteration 82, loss = 0.93020739\n",
      "Iteration 83, loss = 0.92414548\n",
      "Iteration 84, loss = 0.91855027\n",
      "Iteration 85, loss = 0.91393249\n",
      "Iteration 86, loss = 0.90918479\n",
      "Iteration 87, loss = 0.90297072\n",
      "Iteration 88, loss = 0.89814646\n",
      "Iteration 89, loss = 0.89340614\n",
      "Iteration 90, loss = 0.88833072\n",
      "Iteration 91, loss = 0.88271365\n",
      "Iteration 92, loss = 0.87884752\n",
      "Iteration 93, loss = 0.87404878\n",
      "Iteration 94, loss = 0.86731066\n",
      "Iteration 95, loss = 0.86257951\n",
      "Iteration 96, loss = 0.85822313\n",
      "Iteration 97, loss = 0.85226677\n",
      "Iteration 98, loss = 0.84665318\n",
      "Iteration 99, loss = 0.84218904\n",
      "Iteration 100, loss = 0.83636099\n",
      "Iteration 1, loss = 1.84610140\n",
      "Iteration 2, loss = 1.75000666\n",
      "Iteration 3, loss = 1.68420283\n",
      "Iteration 4, loss = 1.62673728\n",
      "Iteration 5, loss = 1.58084959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:542: UserWarning: Got `batch_size` less than 1 or larger than sample size. It is going to be clipped\n",
      "  warnings.warn(\"Got `batch_size` less than 1 or larger than \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6, loss = 1.54703466\n",
      "Iteration 7, loss = 1.52042159\n",
      "Iteration 8, loss = 1.48849750\n",
      "Iteration 9, loss = 1.45657191\n",
      "Iteration 10, loss = 1.42675289\n",
      "Iteration 11, loss = 1.40198164\n",
      "Iteration 12, loss = 1.38381512\n",
      "Iteration 13, loss = 1.37365012\n",
      "Iteration 14, loss = 1.36983550\n",
      "Iteration 15, loss = 1.37038957\n",
      "Iteration 16, loss = 1.37260384\n",
      "Iteration 17, loss = 1.37483694\n",
      "Iteration 18, loss = 1.37583207\n",
      "Iteration 19, loss = 1.37424277\n",
      "Iteration 20, loss = 1.37004843\n",
      "Iteration 21, loss = 1.36432423\n",
      "Iteration 22, loss = 1.35796270\n",
      "Iteration 23, loss = 1.35167902\n",
      "Iteration 24, loss = 1.34616072\n",
      "Iteration 25, loss = 1.34151316\n",
      "Iteration 26, loss = 1.33750752\n",
      "Iteration 27, loss = 1.33400989\n",
      "Iteration 28, loss = 1.33103477\n",
      "Iteration 29, loss = 1.32835953\n",
      "Iteration 30, loss = 1.32582373\n",
      "Iteration 31, loss = 1.32329917\n",
      "Iteration 32, loss = 1.32053785\n",
      "Iteration 33, loss = 1.31728931\n",
      "Iteration 34, loss = 1.31341639\n",
      "Iteration 35, loss = 1.30907981\n",
      "Iteration 36, loss = 1.30465795\n",
      "Iteration 37, loss = 1.30046585\n",
      "Iteration 38, loss = 1.29653271\n",
      "Iteration 39, loss = 1.29266376\n",
      "Iteration 40, loss = 1.28883549\n",
      "Iteration 41, loss = 1.28518896\n",
      "Iteration 42, loss = 1.28159651\n",
      "Iteration 43, loss = 1.27789771\n",
      "Iteration 44, loss = 1.27397540\n",
      "Iteration 45, loss = 1.26976842\n",
      "Iteration 46, loss = 1.26537719\n",
      "Iteration 47, loss = 1.26092327\n",
      "Iteration 48, loss = 1.25642941\n",
      "Iteration 49, loss = 1.25188275\n",
      "Iteration 50, loss = 1.24717954\n",
      "Iteration 51, loss = 1.24257776\n",
      "Iteration 52, loss = 1.23789637\n",
      "Iteration 53, loss = 1.23301822\n",
      "Iteration 54, loss = 1.22808871\n",
      "Iteration 55, loss = 1.22312220\n",
      "Iteration 56, loss = 1.21806241\n",
      "Iteration 57, loss = 1.21290679\n",
      "Iteration 58, loss = 1.20765098\n",
      "Iteration 59, loss = 1.20217153\n",
      "Iteration 60, loss = 1.19654396\n",
      "Iteration 61, loss = 1.19081322\n",
      "Iteration 62, loss = 1.18492499\n",
      "Iteration 63, loss = 1.17899432\n",
      "Iteration 64, loss = 1.17317806\n",
      "Iteration 65, loss = 1.16732687\n",
      "Iteration 66, loss = 1.16142989\n",
      "Iteration 67, loss = 1.15545152\n",
      "Iteration 68, loss = 1.14957431\n",
      "Iteration 69, loss = 1.14370491\n",
      "Iteration 70, loss = 1.13775736\n",
      "Iteration 71, loss = 1.13178515\n",
      "Iteration 72, loss = 1.12587632\n",
      "Iteration 73, loss = 1.11980295\n",
      "Iteration 74, loss = 1.11372367\n",
      "Iteration 75, loss = 1.10760652\n",
      "Iteration 76, loss = 1.10157779\n",
      "Iteration 77, loss = 1.09559336\n",
      "Iteration 78, loss = 1.08946249\n",
      "Iteration 79, loss = 1.08352388\n",
      "Iteration 80, loss = 1.07754345\n",
      "Iteration 81, loss = 1.07151362\n",
      "Iteration 82, loss = 1.06550597\n",
      "Iteration 83, loss = 1.05953779\n",
      "Iteration 84, loss = 1.05368249\n",
      "Iteration 85, loss = 1.04773168\n",
      "Iteration 86, loss = 1.04145645\n",
      "Iteration 87, loss = 1.03515969\n",
      "Iteration 88, loss = 1.02908292\n",
      "Iteration 89, loss = 1.02313789\n",
      "Iteration 90, loss = 1.01687202\n",
      "Iteration 91, loss = 1.01057596\n",
      "Iteration 92, loss = 1.00402942\n",
      "Iteration 93, loss = 0.99779025\n",
      "Iteration 94, loss = 0.99106081\n",
      "Iteration 95, loss = 0.98424135\n",
      "Iteration 96, loss = 0.97737252\n",
      "Iteration 97, loss = 0.97085210\n",
      "Iteration 98, loss = 0.96446625\n",
      "Iteration 99, loss = 0.95801934\n",
      "Iteration 100, loss = 0.95151562\n",
      "Iteration 1, loss = 2.04568813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:542: UserWarning: Got `batch_size` less than 1 or larger than sample size. It is going to be clipped\n",
      "  warnings.warn(\"Got `batch_size` less than 1 or larger than \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 1.82237599\n",
      "Iteration 3, loss = 1.73002573\n",
      "Iteration 4, loss = 1.66188937\n",
      "Iteration 5, loss = 1.60454995\n",
      "Iteration 6, loss = 1.54894863\n",
      "Iteration 7, loss = 1.49662330\n",
      "Iteration 8, loss = 1.46098160\n",
      "Iteration 9, loss = 1.44602085\n",
      "Iteration 10, loss = 1.44901321\n",
      "Iteration 11, loss = 1.46203644\n",
      "Iteration 12, loss = 1.47449821\n",
      "Iteration 13, loss = 1.47908659\n",
      "Iteration 14, loss = 1.47422353\n",
      "Iteration 15, loss = 1.46193127\n",
      "Iteration 16, loss = 1.44566214\n",
      "Iteration 17, loss = 1.42938347\n",
      "Iteration 18, loss = 1.41518739\n",
      "Iteration 19, loss = 1.40488308\n",
      "Iteration 20, loss = 1.40027571\n",
      "Iteration 21, loss = 1.40106018\n",
      "Iteration 22, loss = 1.40064454\n",
      "Iteration 23, loss = 1.39729982\n",
      "Iteration 24, loss = 1.39164627\n",
      "Iteration 25, loss = 1.38368632\n",
      "Iteration 26, loss = 1.37309833\n",
      "Iteration 27, loss = 1.36129751\n",
      "Iteration 28, loss = 1.34969040\n",
      "Iteration 29, loss = 1.34090848\n",
      "Iteration 30, loss = 1.33562959\n",
      "Iteration 31, loss = 1.33169751\n",
      "Iteration 32, loss = 1.32655615\n",
      "Iteration 33, loss = 1.31918192\n",
      "Iteration 34, loss = 1.31045384\n",
      "Iteration 35, loss = 1.30241245\n",
      "Iteration 36, loss = 1.29691983\n",
      "Iteration 37, loss = 1.29406484\n",
      "Iteration 38, loss = 1.29134007\n",
      "Iteration 39, loss = 1.28611905\n",
      "Iteration 40, loss = 1.27911764\n",
      "Iteration 41, loss = 1.27299836\n",
      "Iteration 42, loss = 1.26918579\n",
      "Iteration 43, loss = 1.26613330\n",
      "Iteration 44, loss = 1.26154091\n",
      "Iteration 45, loss = 1.25499460\n",
      "Iteration 46, loss = 1.24866341\n",
      "Iteration 47, loss = 1.24398722\n",
      "Iteration 48, loss = 1.24014830\n",
      "Iteration 49, loss = 1.23521685\n",
      "Iteration 50, loss = 1.22906457\n",
      "Iteration 51, loss = 1.22344253\n",
      "Iteration 52, loss = 1.21903328\n",
      "Iteration 53, loss = 1.21458236\n",
      "Iteration 54, loss = 1.20919117\n",
      "Iteration 55, loss = 1.20369154\n",
      "Iteration 56, loss = 1.19899865\n",
      "Iteration 57, loss = 1.19458164\n",
      "Iteration 58, loss = 1.18953534\n",
      "Iteration 59, loss = 1.18422810\n",
      "Iteration 60, loss = 1.17937069\n",
      "Iteration 61, loss = 1.17459655\n",
      "Iteration 62, loss = 1.16936866\n",
      "Iteration 63, loss = 1.16412753\n",
      "Iteration 64, loss = 1.15923331\n",
      "Iteration 65, loss = 1.15412847\n",
      "Iteration 66, loss = 1.14868888\n",
      "Iteration 67, loss = 1.14354491\n",
      "Iteration 68, loss = 1.13833105\n",
      "Iteration 69, loss = 1.13280374\n",
      "Iteration 70, loss = 1.12739291\n",
      "Iteration 71, loss = 1.12188766\n",
      "Iteration 72, loss = 1.11610934\n",
      "Iteration 73, loss = 1.10964087\n",
      "Iteration 74, loss = 1.10176246\n",
      "Iteration 75, loss = 1.09324994\n",
      "Iteration 76, loss = 1.08429222\n",
      "Iteration 77, loss = 1.07729823\n",
      "Iteration 78, loss = 1.07141973\n",
      "Iteration 79, loss = 1.06478832\n",
      "Iteration 80, loss = 1.05807517\n",
      "Iteration 81, loss = 1.05196916\n",
      "Iteration 82, loss = 1.04507635\n",
      "Iteration 83, loss = 1.03838301\n",
      "Iteration 84, loss = 1.03123248\n",
      "Iteration 85, loss = 1.02433856\n",
      "Iteration 86, loss = 1.01767293\n",
      "Iteration 87, loss = 1.01071054\n",
      "Iteration 88, loss = 1.00405925\n",
      "Iteration 89, loss = 0.99722280\n",
      "Iteration 90, loss = 0.99072129\n",
      "Iteration 91, loss = 0.98382709\n",
      "Iteration 92, loss = 0.97718448\n",
      "Iteration 93, loss = 0.97028030\n",
      "Iteration 94, loss = 0.96373739\n",
      "Iteration 95, loss = 0.95678897\n",
      "Iteration 96, loss = 0.95005612\n",
      "Iteration 97, loss = 0.94320316\n",
      "Iteration 98, loss = 0.93624315\n",
      "Iteration 99, loss = 0.92946335\n",
      "Iteration 100, loss = 0.92244089\n",
      "0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(mlpc, xtest, ytest, cv=5)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6518566139254778"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.roc_auc_score(ytest,mlpc.predict_proba(xtest),multi_class='ovr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Voting] ...................... (1 of 4) Processing RFC, total=   1.2s\n",
      "[Voting] ...................... (2 of 4) Processing LRC, total=   0.2s\n",
      "[Voting] ...................... (3 of 4) Processing ABC, total=   2.1s\n",
      "Iteration 1, loss = 1.61869349\n",
      "Iteration 2, loss = 1.50728424\n",
      "Iteration 3, loss = 1.46766474\n",
      "Iteration 4, loss = 1.45580319\n",
      "Iteration 5, loss = 1.45165926\n",
      "Iteration 6, loss = 1.43817912\n",
      "Iteration 7, loss = 1.43230696\n",
      "Iteration 8, loss = 1.41981408\n",
      "Iteration 9, loss = 1.41323166\n",
      "Iteration 10, loss = 1.40590235\n",
      "Iteration 11, loss = 1.39887260\n",
      "Iteration 12, loss = 1.39133062\n",
      "Iteration 13, loss = 1.38394732\n",
      "Iteration 14, loss = 1.37809700\n",
      "Iteration 15, loss = 1.37161196\n",
      "Iteration 16, loss = 1.36134417\n",
      "Iteration 17, loss = 1.36243942\n",
      "Iteration 18, loss = 1.35574986\n",
      "Iteration 19, loss = 1.36024667\n",
      "Iteration 20, loss = 1.35647220\n",
      "Iteration 21, loss = 1.34597480\n",
      "Iteration 22, loss = 1.34696831\n",
      "Iteration 23, loss = 1.33285758\n",
      "Iteration 24, loss = 1.33349854\n",
      "Iteration 25, loss = 1.32814471\n",
      "Iteration 26, loss = 1.31818136\n",
      "Iteration 27, loss = 1.31634194\n",
      "Iteration 28, loss = 1.31639144\n",
      "Iteration 29, loss = 1.31288837\n",
      "Iteration 30, loss = 1.30813679\n",
      "Iteration 31, loss = 1.30851485\n",
      "Iteration 32, loss = 1.30275779\n",
      "Iteration 33, loss = 1.29669546\n",
      "Iteration 34, loss = 1.29888794\n",
      "Iteration 35, loss = 1.29270039\n",
      "Iteration 36, loss = 1.29215190\n",
      "Iteration 37, loss = 1.28550511\n",
      "Iteration 38, loss = 1.28387407\n",
      "Iteration 39, loss = 1.30416498\n",
      "Iteration 40, loss = 1.27962840\n",
      "Iteration 41, loss = 1.28550449\n",
      "Iteration 42, loss = 1.28588135\n",
      "Iteration 43, loss = 1.26931009\n",
      "Iteration 44, loss = 1.27675569\n",
      "Iteration 45, loss = 1.26696015\n",
      "Iteration 46, loss = 1.26252771\n",
      "Iteration 47, loss = 1.27188370\n",
      "Iteration 48, loss = 1.26190972\n",
      "Iteration 49, loss = 1.25294463\n",
      "Iteration 50, loss = 1.27268186\n",
      "Iteration 51, loss = 1.25879961\n",
      "Iteration 52, loss = 1.25123808\n",
      "Iteration 53, loss = 1.26474701\n",
      "Iteration 54, loss = 1.25334203\n",
      "Iteration 55, loss = 1.24134996\n",
      "Iteration 56, loss = 1.24722275\n",
      "Iteration 57, loss = 1.23597996\n",
      "Iteration 58, loss = 1.23399443\n",
      "Iteration 59, loss = 1.23921548\n",
      "Iteration 60, loss = 1.23051705\n",
      "Iteration 61, loss = 1.22280730\n",
      "Iteration 62, loss = 1.22679987\n",
      "Iteration 63, loss = 1.22047788\n",
      "Iteration 64, loss = 1.22142855\n",
      "Iteration 65, loss = 1.21122009\n",
      "Iteration 66, loss = 1.21984304\n",
      "Iteration 67, loss = 1.20897139\n",
      "Iteration 68, loss = 1.20520319\n",
      "Iteration 69, loss = 1.20079009\n",
      "Iteration 70, loss = 1.21578838\n",
      "Iteration 71, loss = 1.20583192\n",
      "Iteration 72, loss = 1.19542415\n",
      "Iteration 73, loss = 1.20049549\n",
      "Iteration 74, loss = 1.18995817\n",
      "Iteration 75, loss = 1.18233594\n",
      "Iteration 76, loss = 1.18597755\n",
      "Iteration 77, loss = 1.18137103\n",
      "Iteration 78, loss = 1.17451444\n",
      "Iteration 79, loss = 1.16804190\n",
      "Iteration 80, loss = 1.17888298\n",
      "Iteration 81, loss = 1.16566460\n",
      "Iteration 82, loss = 1.16600214\n",
      "Iteration 83, loss = 1.16388455\n",
      "Iteration 84, loss = 1.15838419\n",
      "Iteration 85, loss = 1.15902929\n",
      "Iteration 86, loss = 1.14978273\n",
      "Iteration 87, loss = 1.14690688\n",
      "Iteration 88, loss = 1.14579119\n",
      "Iteration 89, loss = 1.13977267\n",
      "Iteration 90, loss = 1.13894569\n",
      "Iteration 91, loss = 1.14001686\n",
      "Iteration 92, loss = 1.14237136\n",
      "Iteration 93, loss = 1.13696399\n",
      "Iteration 94, loss = 1.12963105\n",
      "Iteration 95, loss = 1.13042141\n",
      "Iteration 96, loss = 1.12472143\n",
      "Iteration 97, loss = 1.11890840\n",
      "Iteration 98, loss = 1.11571037\n",
      "Iteration 99, loss = 1.11178941\n",
      "Iteration 100, loss = 1.10170644\n",
      "[Voting] ...................... (4 of 4) Processing MLP, total=   4.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('RFC',\n",
       "                              RandomForestClassifier(max_depth=50,\n",
       "                                                     max_features=10,\n",
       "                                                     min_samples_leaf=10,\n",
       "                                                     min_samples_split=10,\n",
       "                                                     n_estimators=250)),\n",
       "                             ('LRC',\n",
       "                              LogisticRegression(max_iter=1000,\n",
       "                                                 random_state=0)),\n",
       "                             ('ABC',\n",
       "                              AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=10,\n",
       "                                                                                       min_samples_split=64),\n",
       "                                                 learning_rate=0.5,\n",
       "                                                 n_estimators=225)),\n",
       "                             ('MLP',\n",
       "                              MLPClassifier(batch_size=256,\n",
       "                                            hidden_layer_sizes=(1000, 100, 10),\n",
       "                                            max_iter=100, verbose=True))],\n",
       "                 verbose=True, voting='soft')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier \n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "estimator = [] \n",
    "estimator.append(('RFC',rf))\n",
    "estimator.append(('LRC',clf))\n",
    "estimator.append(('ABC',abc))\n",
    "estimator.append(('MLP',mlpc))\n",
    "\n",
    "\n",
    "vot_soft = VotingClassifier(estimators = estimator, voting='soft', verbose=True) \n",
    "vot_soft.fit(xtrain, ytrain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Voting] ...................... (1 of 4) Processing RFC, total=   0.5s\n",
      "[Voting] ...................... (2 of 4) Processing LRC, total=   0.0s\n",
      "[Voting] ...................... (3 of 4) Processing ABC, total=   0.8s\n",
      "Iteration 1, loss = 1.75993800\n",
      "Iteration 2, loss = 1.69954905\n",
      "Iteration 3, loss = 1.64381591\n",
      "Iteration 4, loss = 1.59540861\n",
      "Iteration 5, loss = 1.54027855\n",
      "Iteration 6, loss = 1.48993179\n",
      "Iteration 7, loss = 1.44919962\n",
      "Iteration 8, loss = 1.42389757\n",
      "Iteration 9, loss = 1.41331692\n",
      "Iteration 10, loss = 1.41185968\n",
      "Iteration 11, loss = 1.41222886\n",
      "Iteration 12, loss = 1.40859316\n",
      "Iteration 13, loss = 1.39919420\n",
      "Iteration 14, loss = 1.38614700\n",
      "Iteration 15, loss = 1.37384027\n",
      "Iteration 16, loss = 1.36400101\n",
      "Iteration 17, loss = 1.35706679\n",
      "Iteration 18, loss = 1.35242364\n",
      "Iteration 19, loss = 1.34739892"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:542: UserWarning: Got `batch_size` less than 1 or larger than sample size. It is going to be clipped\n",
      "  warnings.warn(\"Got `batch_size` less than 1 or larger than \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 20, loss = 1.34137373\n",
      "Iteration 21, loss = 1.33528482\n",
      "Iteration 22, loss = 1.33021608\n",
      "Iteration 23, loss = 1.32550079\n",
      "Iteration 24, loss = 1.32121410\n",
      "Iteration 25, loss = 1.31616974\n",
      "Iteration 26, loss = 1.31054167\n",
      "Iteration 27, loss = 1.30533260\n",
      "Iteration 28, loss = 1.30111550\n",
      "Iteration 29, loss = 1.29677740\n",
      "Iteration 30, loss = 1.29213289\n",
      "Iteration 31, loss = 1.28680518\n",
      "Iteration 32, loss = 1.28119208\n",
      "Iteration 33, loss = 1.27617338\n",
      "Iteration 34, loss = 1.27164616\n",
      "Iteration 35, loss = 1.26657061\n",
      "Iteration 36, loss = 1.26044822\n",
      "Iteration 37, loss = 1.25429850\n",
      "Iteration 38, loss = 1.24883851\n",
      "Iteration 39, loss = 1.24331743\n",
      "Iteration 40, loss = 1.23715542\n",
      "Iteration 41, loss = 1.23080616\n",
      "Iteration 42, loss = 1.22465180\n",
      "Iteration 43, loss = 1.21853453\n",
      "Iteration 44, loss = 1.21228397\n",
      "Iteration 45, loss = 1.20560071\n",
      "Iteration 46, loss = 1.19880050\n",
      "Iteration 47, loss = 1.19244749\n",
      "Iteration 48, loss = 1.18589034\n",
      "Iteration 49, loss = 1.17897758\n",
      "Iteration 50, loss = 1.17224238\n",
      "Iteration 51, loss = 1.16549009\n",
      "Iteration 52, loss = 1.15848761\n",
      "Iteration 53, loss = 1.15124008\n",
      "Iteration 54, loss = 1.14395084\n",
      "Iteration 55, loss = 1.13657945\n",
      "Iteration 56, loss = 1.12892864\n",
      "Iteration 57, loss = 1.12122038\n",
      "Iteration 58, loss = 1.11343772\n",
      "Iteration 59, loss = 1.10544913\n",
      "Iteration 60, loss = 1.09738520\n",
      "Iteration 61, loss = 1.08934960\n",
      "Iteration 62, loss = 1.08117811\n",
      "Iteration 63, loss = 1.07282042\n",
      "Iteration 64, loss = 1.06409720\n",
      "Iteration 65, loss = 1.05509922\n",
      "Iteration 66, loss = 1.04612389\n",
      "Iteration 67, loss = 1.03712649\n",
      "Iteration 68, loss = 1.02799579\n",
      "Iteration 69, loss = 1.01880003\n",
      "Iteration 70, loss = 1.00972072\n",
      "Iteration 71, loss = 1.00043013\n",
      "Iteration 72, loss = 0.99100627\n",
      "Iteration 73, loss = 0.98172820\n",
      "Iteration 74, loss = 0.97244420\n",
      "Iteration 75, loss = 0.96299851\n",
      "Iteration 76, loss = 0.95350854\n",
      "Iteration 77, loss = 0.94404259\n",
      "Iteration 78, loss = 0.93449833\n",
      "Iteration 79, loss = 0.92495070\n",
      "Iteration 80, loss = 0.91547455\n",
      "Iteration 81, loss = 0.90590560\n",
      "Iteration 82, loss = 0.89645749\n",
      "Iteration 83, loss = 0.88690465\n",
      "Iteration 84, loss = 0.87723737\n",
      "Iteration 85, loss = 0.86742280\n",
      "Iteration 86, loss = 0.85784583\n",
      "Iteration 87, loss = 0.84812275\n",
      "Iteration 88, loss = 0.83832692\n",
      "Iteration 89, loss = 0.82870637\n",
      "Iteration 90, loss = 0.81918341\n",
      "Iteration 91, loss = 0.80951584\n",
      "Iteration 92, loss = 0.79983602\n",
      "Iteration 93, loss = 0.79022195\n",
      "Iteration 94, loss = 0.78047862\n",
      "Iteration 95, loss = 0.77059018\n",
      "Iteration 96, loss = 0.76094940\n",
      "Iteration 97, loss = 0.75143840\n",
      "Iteration 98, loss = 0.74198949\n",
      "Iteration 99, loss = 0.73313452\n",
      "Iteration 100, loss = 0.72433357\n",
      "[Voting] ...................... (4 of 4) Processing MLP, total=   1.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Voting] ...................... (1 of 4) Processing RFC, total=   0.6s\n",
      "[Voting] ...................... (2 of 4) Processing LRC, total=   0.0s\n",
      "[Voting] ...................... (3 of 4) Processing ABC, total=   0.6s\n",
      "Iteration 1, loss = 1.67146281\n",
      "Iteration 2, loss = 1.59487035\n",
      "Iteration 3, loss = 1.53880336\n",
      "Iteration 4, loss = 1.49291278\n",
      "Iteration 5, loss = 1.45916096\n",
      "Iteration 6, loss = 1.43845257\n",
      "Iteration 7, loss = 1.43158915\n",
      "Iteration 8, loss = 1.43594412\n",
      "Iteration 9, loss = 1.44372902\n",
      "Iteration 10, loss = 1.44715103\n",
      "Iteration 11, loss = 1.44354307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:542: UserWarning: Got `batch_size` less than 1 or larger than sample size. It is going to be clipped\n",
      "  warnings.warn(\"Got `batch_size` less than 1 or larger than \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12, loss = 1.43471287\n",
      "Iteration 13, loss = 1.42387245\n",
      "Iteration 14, loss = 1.41403955\n",
      "Iteration 15, loss = 1.40729699\n",
      "Iteration 16, loss = 1.40384984\n",
      "Iteration 17, loss = 1.40280832\n",
      "Iteration 18, loss = 1.40238831\n",
      "Iteration 19, loss = 1.40119173\n",
      "Iteration 20, loss = 1.39834768\n",
      "Iteration 21, loss = 1.39365574\n",
      "Iteration 22, loss = 1.38774904\n",
      "Iteration 23, loss = 1.38170390\n",
      "Iteration 24, loss = 1.37644052\n",
      "Iteration 25, loss = 1.37258701\n",
      "Iteration 26, loss = 1.36994176\n",
      "Iteration 27, loss = 1.36773012\n",
      "Iteration 28, loss = 1.36484323\n",
      "Iteration 29, loss = 1.36073276\n",
      "Iteration 30, loss = 1.35555318\n",
      "Iteration 31, loss = 1.35017739\n",
      "Iteration 32, loss = 1.34601543\n",
      "Iteration 33, loss = 1.34305510\n",
      "Iteration 34, loss = 1.33948807\n",
      "Iteration 35, loss = 1.33466092\n",
      "Iteration 36, loss = 1.32923316\n",
      "Iteration 37, loss = 1.32402932\n",
      "Iteration 38, loss = 1.31928067\n",
      "Iteration 39, loss = 1.31402599\n",
      "Iteration 40, loss = 1.30747421\n",
      "Iteration 41, loss = 1.30000700\n",
      "Iteration 42, loss = 1.29245054\n",
      "Iteration 43, loss = 1.28648691\n",
      "Iteration 44, loss = 1.28061716\n",
      "Iteration 45, loss = 1.27397198\n",
      "Iteration 46, loss = 1.26802818\n",
      "Iteration 47, loss = 1.26307993\n",
      "Iteration 48, loss = 1.25746964\n",
      "Iteration 49, loss = 1.25087743\n",
      "Iteration 50, loss = 1.24546147\n",
      "Iteration 51, loss = 1.24065599\n",
      "Iteration 52, loss = 1.23531122\n",
      "Iteration 53, loss = 1.22989727\n",
      "Iteration 54, loss = 1.22490312\n",
      "Iteration 55, loss = 1.21946533\n",
      "Iteration 56, loss = 1.21454169\n",
      "Iteration 57, loss = 1.20984551\n",
      "Iteration 58, loss = 1.20491040\n",
      "Iteration 59, loss = 1.20012033\n",
      "Iteration 60, loss = 1.19523807\n",
      "Iteration 61, loss = 1.19020252\n",
      "Iteration 62, loss = 1.18529588\n",
      "Iteration 63, loss = 1.18041355\n",
      "Iteration 64, loss = 1.17525935\n",
      "Iteration 65, loss = 1.17024563\n",
      "Iteration 66, loss = 1.16522186\n",
      "Iteration 67, loss = 1.16029737\n",
      "Iteration 68, loss = 1.15502062\n",
      "Iteration 69, loss = 1.14985285\n",
      "Iteration 70, loss = 1.14417713\n",
      "Iteration 71, loss = 1.13850969\n",
      "Iteration 72, loss = 1.13266872\n",
      "Iteration 73, loss = 1.12702221\n",
      "Iteration 74, loss = 1.12091752\n",
      "Iteration 75, loss = 1.11469983\n",
      "Iteration 76, loss = 1.10863361\n",
      "Iteration 77, loss = 1.10232065\n",
      "Iteration 78, loss = 1.09482954\n",
      "Iteration 79, loss = 1.08812013\n",
      "Iteration 80, loss = 1.08104955\n",
      "Iteration 81, loss = 1.07371387\n",
      "Iteration 82, loss = 1.06677717\n",
      "Iteration 83, loss = 1.05972858\n",
      "Iteration 84, loss = 1.05260167\n",
      "Iteration 85, loss = 1.04544418\n",
      "Iteration 86, loss = 1.03822110\n",
      "Iteration 87, loss = 1.03084716\n",
      "Iteration 88, loss = 1.02332247\n",
      "Iteration 89, loss = 1.01565124\n",
      "Iteration 90, loss = 1.00838774\n",
      "Iteration 91, loss = 1.00062487\n",
      "Iteration 92, loss = 0.99315957\n",
      "Iteration 93, loss = 0.98447869\n",
      "Iteration 94, loss = 0.97644228\n",
      "Iteration 95, loss = 0.96956394\n",
      "Iteration 96, loss = 0.96206673\n",
      "Iteration 97, loss = 0.95403376\n",
      "Iteration 98, loss = 0.94574057\n",
      "Iteration 99, loss = 0.93729808\n",
      "Iteration 100, loss = 0.93126313\n",
      "[Voting] ...................... (4 of 4) Processing MLP, total=   1.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Voting] ...................... (1 of 4) Processing RFC, total=   0.7s\n",
      "[Voting] ...................... (2 of 4) Processing LRC, total=   0.0s\n",
      "[Voting] ...................... (3 of 4) Processing ABC, total=   0.6s\n",
      "Iteration 1, loss = 1.80981773\n",
      "Iteration 2, loss = 1.71756601\n",
      "Iteration 3, loss = 1.66012370\n",
      "Iteration 4, loss = 1.61113269\n",
      "Iteration 5, loss = 1.56931412\n",
      "Iteration 6, loss = 1.53547935\n",
      "Iteration 7, loss = 1.50782178\n",
      "Iteration 8, loss = 1.48519568\n",
      "Iteration 9, loss = 1.46728034\n",
      "Iteration 10, loss = 1.45338820\n",
      "Iteration 11, loss = 1.44393611\n",
      "Iteration 12, loss = 1.43854120\n",
      "Iteration 13, loss = 1.43562748\n",
      "Iteration 14, loss = 1.43341851\n",
      "Iteration 15, loss = 1.42971694\n",
      "Iteration 16, loss = 1.42275251\n",
      "Iteration 17, loss = 1.41249058\n",
      "Iteration 18, loss = 1.40018854\n",
      "Iteration 19, loss = 1.38735933\n",
      "Iteration 20, loss = 1.37538292\n",
      "Iteration 21, loss = 1.36524187\n",
      "Iteration 22, loss = 1.35705503\n",
      "Iteration 23, loss = 1.35051376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:542: UserWarning: Got `batch_size` less than 1 or larger than sample size. It is going to be clipped\n",
      "  warnings.warn(\"Got `batch_size` less than 1 or larger than \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 24, loss = 1.34517939\n",
      "Iteration 25, loss = 1.34074076\n",
      "Iteration 26, loss = 1.33653520\n",
      "Iteration 27, loss = 1.33224602\n",
      "Iteration 28, loss = 1.32760435\n",
      "Iteration 29, loss = 1.32267015\n",
      "Iteration 30, loss = 1.31789404\n",
      "Iteration 31, loss = 1.31381200\n",
      "Iteration 32, loss = 1.31053381\n",
      "Iteration 33, loss = 1.30754531\n",
      "Iteration 34, loss = 1.30442990\n",
      "Iteration 35, loss = 1.30117997\n",
      "Iteration 36, loss = 1.29773319\n",
      "Iteration 37, loss = 1.29377479\n",
      "Iteration 38, loss = 1.28936479\n",
      "Iteration 39, loss = 1.28487761\n",
      "Iteration 40, loss = 1.28045719\n",
      "Iteration 41, loss = 1.27636399\n",
      "Iteration 42, loss = 1.27229596\n",
      "Iteration 43, loss = 1.26807675\n",
      "Iteration 44, loss = 1.26385449\n",
      "Iteration 45, loss = 1.25968403\n",
      "Iteration 46, loss = 1.25545915\n",
      "Iteration 47, loss = 1.25126949\n",
      "Iteration 48, loss = 1.24722111\n",
      "Iteration 49, loss = 1.24307152\n",
      "Iteration 50, loss = 1.23898921\n",
      "Iteration 51, loss = 1.23486952\n",
      "Iteration 52, loss = 1.23054849\n",
      "Iteration 53, loss = 1.22619666\n",
      "Iteration 54, loss = 1.22176289\n",
      "Iteration 55, loss = 1.21731553\n",
      "Iteration 56, loss = 1.21282992\n",
      "Iteration 57, loss = 1.20831150\n",
      "Iteration 58, loss = 1.20377992\n",
      "Iteration 59, loss = 1.19921756\n",
      "Iteration 60, loss = 1.19454561\n",
      "Iteration 61, loss = 1.19025237\n",
      "Iteration 62, loss = 1.18547281\n",
      "Iteration 63, loss = 1.18095622\n",
      "Iteration 64, loss = 1.17654630\n",
      "Iteration 65, loss = 1.17159848\n",
      "Iteration 66, loss = 1.16721286\n",
      "Iteration 67, loss = 1.16235950\n",
      "Iteration 68, loss = 1.15754005\n",
      "Iteration 69, loss = 1.15293727\n",
      "Iteration 70, loss = 1.14793534\n",
      "Iteration 71, loss = 1.14308966\n",
      "Iteration 72, loss = 1.13821776\n",
      "Iteration 73, loss = 1.13330717\n",
      "Iteration 74, loss = 1.12844943\n",
      "Iteration 75, loss = 1.12360872\n",
      "Iteration 76, loss = 1.11861042\n",
      "Iteration 77, loss = 1.11374753\n",
      "Iteration 78, loss = 1.10882787\n",
      "Iteration 79, loss = 1.10359183\n",
      "Iteration 80, loss = 1.09840523\n",
      "Iteration 81, loss = 1.09308946\n",
      "Iteration 82, loss = 1.08790483\n",
      "Iteration 83, loss = 1.08273495\n",
      "Iteration 84, loss = 1.07758671\n",
      "Iteration 85, loss = 1.07264573\n",
      "Iteration 86, loss = 1.06736030\n",
      "Iteration 87, loss = 1.06188412\n",
      "Iteration 88, loss = 1.05636644\n",
      "Iteration 89, loss = 1.05106684\n",
      "Iteration 90, loss = 1.04575931\n",
      "Iteration 91, loss = 1.04057448\n",
      "Iteration 92, loss = 1.03513424\n",
      "Iteration 93, loss = 1.02979569\n",
      "Iteration 94, loss = 1.02443495\n",
      "Iteration 95, loss = 1.01904197\n",
      "Iteration 96, loss = 1.01335071\n",
      "Iteration 97, loss = 1.00772416\n",
      "Iteration 98, loss = 1.00261734\n",
      "Iteration 99, loss = 0.99711138\n",
      "Iteration 100, loss = 0.99145116\n",
      "[Voting] ...................... (4 of 4) Processing MLP, total=   0.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Voting] ...................... (1 of 4) Processing RFC, total=   0.4s\n",
      "[Voting] ...................... (2 of 4) Processing LRC, total=   0.0s\n",
      "[Voting] ...................... (3 of 4) Processing ABC, total=   0.5s\n",
      "Iteration 1, loss = 1.74551666\n",
      "Iteration 2, loss = 1.61873693\n",
      "Iteration 3, loss = 1.57710405\n",
      "Iteration 4, loss = 1.55331410\n",
      "Iteration 5, loss = 1.53294968\n",
      "Iteration 6, loss = 1.51718126\n",
      "Iteration 7, loss = 1.50496388\n",
      "Iteration 8, loss = 1.49541366\n",
      "Iteration 9, loss = 1.48733324\n",
      "Iteration 10, loss = 1.47992633\n",
      "Iteration 11, loss = 1.47215688\n",
      "Iteration 12, loss = 1.46448555\n",
      "Iteration 13, loss = 1.45894721\n",
      "Iteration 14, loss = 1.45474793\n",
      "Iteration 15, loss = 1.45139432\n",
      "Iteration 16, loss = 1.44805781\n",
      "Iteration 17, loss = 1.44439477\n",
      "Iteration 18, loss = 1.44003245\n",
      "Iteration 19, loss = 1.43532718\n",
      "Iteration 20, loss = 1.43090522\n",
      "Iteration 21, loss = 1.42650246\n",
      "Iteration 22, loss = 1.42196720\n",
      "Iteration 23, loss = 1.41760161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:542: UserWarning: Got `batch_size` less than 1 or larger than sample size. It is going to be clipped\n",
      "  warnings.warn(\"Got `batch_size` less than 1 or larger than \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 24, loss = 1.41394317\n",
      "Iteration 25, loss = 1.40996321\n",
      "Iteration 26, loss = 1.40541548\n",
      "Iteration 27, loss = 1.40061611\n",
      "Iteration 28, loss = 1.39565690\n",
      "Iteration 29, loss = 1.39076980\n",
      "Iteration 30, loss = 1.38595680\n",
      "Iteration 31, loss = 1.38111417\n",
      "Iteration 32, loss = 1.37615396\n",
      "Iteration 33, loss = 1.37173080\n",
      "Iteration 34, loss = 1.36708269\n",
      "Iteration 35, loss = 1.36269275\n",
      "Iteration 36, loss = 1.35795459\n",
      "Iteration 37, loss = 1.35281035\n",
      "Iteration 38, loss = 1.34792782\n",
      "Iteration 39, loss = 1.34319519\n",
      "Iteration 40, loss = 1.33843441\n",
      "Iteration 41, loss = 1.33398057\n",
      "Iteration 42, loss = 1.32953133\n",
      "Iteration 43, loss = 1.32491754\n",
      "Iteration 44, loss = 1.32040379\n",
      "Iteration 45, loss = 1.31589958\n",
      "Iteration 46, loss = 1.31140357\n",
      "Iteration 47, loss = 1.30697316\n",
      "Iteration 48, loss = 1.30260621\n",
      "Iteration 49, loss = 1.29822742\n",
      "Iteration 50, loss = 1.29396612\n",
      "Iteration 51, loss = 1.28946088\n",
      "Iteration 52, loss = 1.28527013\n",
      "Iteration 53, loss = 1.28120135\n",
      "Iteration 54, loss = 1.27640541\n",
      "Iteration 55, loss = 1.27207394\n",
      "Iteration 56, loss = 1.26765538\n",
      "Iteration 57, loss = 1.26318079\n",
      "Iteration 58, loss = 1.25882258\n",
      "Iteration 59, loss = 1.25419944\n",
      "Iteration 60, loss = 1.24946540\n",
      "Iteration 61, loss = 1.24450691\n",
      "Iteration 62, loss = 1.23950842\n",
      "Iteration 63, loss = 1.23469487\n",
      "Iteration 64, loss = 1.22956074\n",
      "Iteration 65, loss = 1.22431540\n",
      "Iteration 66, loss = 1.21926981\n",
      "Iteration 67, loss = 1.21377389\n",
      "Iteration 68, loss = 1.20861786\n",
      "Iteration 69, loss = 1.20298592\n",
      "Iteration 70, loss = 1.19760606\n",
      "Iteration 71, loss = 1.19200956\n",
      "Iteration 72, loss = 1.18611419\n",
      "Iteration 73, loss = 1.17996481\n",
      "Iteration 74, loss = 1.17390812\n",
      "Iteration 75, loss = 1.16856425\n",
      "Iteration 76, loss = 1.16287929\n",
      "Iteration 77, loss = 1.15676035\n",
      "Iteration 78, loss = 1.15098770\n",
      "Iteration 79, loss = 1.14519184\n",
      "Iteration 80, loss = 1.13934233\n",
      "Iteration 81, loss = 1.13341375\n",
      "Iteration 82, loss = 1.12769315\n",
      "Iteration 83, loss = 1.12143017\n",
      "Iteration 84, loss = 1.11584790\n",
      "Iteration 85, loss = 1.11018115\n",
      "Iteration 86, loss = 1.10350935\n",
      "Iteration 87, loss = 1.09826893\n",
      "Iteration 88, loss = 1.09159594\n",
      "Iteration 89, loss = 1.08615921\n",
      "Iteration 90, loss = 1.07947301\n",
      "Iteration 91, loss = 1.07321266\n",
      "Iteration 92, loss = 1.06704310\n",
      "Iteration 93, loss = 1.06006186\n",
      "Iteration 94, loss = 1.05395146\n",
      "Iteration 95, loss = 1.04729841\n",
      "Iteration 96, loss = 1.04041315\n",
      "Iteration 97, loss = 1.03279186\n",
      "Iteration 98, loss = 1.02506891\n",
      "Iteration 99, loss = 1.01681533\n",
      "Iteration 100, loss = 1.00835819\n",
      "[Voting] ...................... (4 of 4) Processing MLP, total=   1.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Voting] ...................... (1 of 4) Processing RFC, total=   0.7s\n",
      "[Voting] ...................... (2 of 4) Processing LRC, total=   0.1s\n",
      "[Voting] ...................... (3 of 4) Processing ABC, total=   0.8s\n",
      "Iteration 1, loss = 1.66569489\n",
      "Iteration 2, loss = 1.62205785\n",
      "Iteration 3, loss = 1.61529372\n",
      "Iteration 4, loss = 1.60694166\n",
      "Iteration 5, loss = 1.59246161\n",
      "Iteration 6, loss = 1.56095115\n",
      "Iteration 7, loss = 1.51981082\n",
      "Iteration 8, loss = 1.47954838\n",
      "Iteration 9, loss = 1.44606737\n",
      "Iteration 10, loss = 1.42072469\n",
      "Iteration 11, loss = 1.40445696\n",
      "Iteration 12, loss = 1.39734251\n",
      "Iteration 13, loss = 1.39738778\n",
      "Iteration 14, loss = 1.40087116\n",
      "Iteration 15, loss = 1.40379682\n",
      "Iteration 16, loss = 1.40352172\n",
      "Iteration 17, loss = 1.39940738\n",
      "Iteration 18, loss = 1.39220067\n",
      "Iteration 19, loss = 1.38341821\n",
      "Iteration 20, loss = 1.37469407\n",
      "Iteration 21, loss = 1.36741469\n",
      "Iteration 22, loss = 1.36257866\n",
      "Iteration 23, loss = 1.35984209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:542: UserWarning: Got `batch_size` less than 1 or larger than sample size. It is going to be clipped\n",
      "  warnings.warn(\"Got `batch_size` less than 1 or larger than \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 24, loss = 1.35848466\n",
      "Iteration 25, loss = 1.35761835\n",
      "Iteration 26, loss = 1.35620518\n",
      "Iteration 27, loss = 1.35356139\n",
      "Iteration 28, loss = 1.34954428\n",
      "Iteration 29, loss = 1.34446893\n",
      "Iteration 30, loss = 1.33889645\n",
      "Iteration 31, loss = 1.33334784\n",
      "Iteration 32, loss = 1.32812736\n",
      "Iteration 33, loss = 1.32383613\n",
      "Iteration 34, loss = 1.32026310\n",
      "Iteration 35, loss = 1.31712536\n",
      "Iteration 36, loss = 1.31379602\n",
      "Iteration 37, loss = 1.30997359\n",
      "Iteration 38, loss = 1.30563702\n",
      "Iteration 39, loss = 1.30097669\n",
      "Iteration 40, loss = 1.29620071\n",
      "Iteration 41, loss = 1.29139165\n",
      "Iteration 42, loss = 1.28642477\n",
      "Iteration 43, loss = 1.28144908\n",
      "Iteration 44, loss = 1.27661104\n",
      "Iteration 45, loss = 1.27181599\n",
      "Iteration 46, loss = 1.26719593\n",
      "Iteration 47, loss = 1.26247035\n",
      "Iteration 48, loss = 1.25754536\n",
      "Iteration 49, loss = 1.25237049\n",
      "Iteration 50, loss = 1.24718639\n",
      "Iteration 51, loss = 1.24201983\n",
      "Iteration 52, loss = 1.23700960\n",
      "Iteration 53, loss = 1.23202605\n",
      "Iteration 54, loss = 1.22701115\n",
      "Iteration 55, loss = 1.22198653\n",
      "Iteration 56, loss = 1.21708260\n",
      "Iteration 57, loss = 1.21233707\n",
      "Iteration 58, loss = 1.20753356\n",
      "Iteration 59, loss = 1.20269459\n",
      "Iteration 60, loss = 1.19784728\n",
      "Iteration 61, loss = 1.19311527\n",
      "Iteration 62, loss = 1.18848606\n",
      "Iteration 63, loss = 1.18398925\n",
      "Iteration 64, loss = 1.17949969\n",
      "Iteration 65, loss = 1.17519811\n",
      "Iteration 66, loss = 1.17065518\n",
      "Iteration 67, loss = 1.16629757\n",
      "Iteration 68, loss = 1.16197087\n",
      "Iteration 69, loss = 1.15748011\n",
      "Iteration 70, loss = 1.15297642\n",
      "Iteration 71, loss = 1.14843202\n",
      "Iteration 72, loss = 1.14369532\n",
      "Iteration 73, loss = 1.13891436\n",
      "Iteration 74, loss = 1.13419126\n",
      "Iteration 75, loss = 1.12945467\n",
      "Iteration 76, loss = 1.12472035\n",
      "Iteration 77, loss = 1.11992160\n",
      "Iteration 78, loss = 1.11515755\n",
      "Iteration 79, loss = 1.11019561\n",
      "Iteration 80, loss = 1.10525548\n",
      "Iteration 81, loss = 1.10031526\n",
      "Iteration 82, loss = 1.09536019\n",
      "Iteration 83, loss = 1.09042812\n",
      "Iteration 84, loss = 1.08542364\n",
      "Iteration 85, loss = 1.08079463\n",
      "Iteration 86, loss = 1.07545351\n",
      "Iteration 87, loss = 1.07016436\n",
      "Iteration 88, loss = 1.06556281\n",
      "Iteration 89, loss = 1.05992018\n",
      "Iteration 90, loss = 1.05498278\n",
      "Iteration 91, loss = 1.04977464\n",
      "Iteration 92, loss = 1.04421816\n",
      "Iteration 93, loss = 1.03949784\n",
      "Iteration 94, loss = 1.03389056\n",
      "Iteration 95, loss = 1.02944395\n",
      "Iteration 96, loss = 1.02367352\n",
      "Iteration 97, loss = 1.01789200\n",
      "Iteration 98, loss = 1.01268522\n",
      "Iteration 99, loss = 1.00661257\n",
      "Iteration 100, loss = 1.00079713\n",
      "[Voting] ...................... (4 of 4) Processing MLP, total=   1.6s\n",
      "0.4935483870967742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(vot_soft, xtest, ytest, cv=5)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6271242814788651"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.roc_auc_score(ytest,vot_soft.predict_proba(xtest),multi_class='ovr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on Veterans of 2021-2022 Season"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Cleaning for NBA 2021-2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = pd.read_csv('compiled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data[data['Year']==2021]\n",
    "data2 = data2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Player</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tm</th>\n",
       "      <th>G</th>\n",
       "      <th>MP</th>\n",
       "      <th>PER</th>\n",
       "      <th>TS%</th>\n",
       "      <th>3PAr</th>\n",
       "      <th>...</th>\n",
       "      <th>OWS</th>\n",
       "      <th>DWS</th>\n",
       "      <th>WS</th>\n",
       "      <th>WS/48</th>\n",
       "      <th>OBPM</th>\n",
       "      <th>DBPM</th>\n",
       "      <th>3P%</th>\n",
       "      <th>2P%</th>\n",
       "      <th>FT%</th>\n",
       "      <th>TmNetRtg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1423</th>\n",
       "      <td>2021</td>\n",
       "      <td>Carmelo Anthony</td>\n",
       "      <td>PF</td>\n",
       "      <td>36.0</td>\n",
       "      <td>POR</td>\n",
       "      <td>69.0</td>\n",
       "      <td>1690.0</td>\n",
       "      <td>14.6</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.418</td>\n",
       "      <td>...</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-1.6</td>\n",
       "      <td>0.409</td>\n",
       "      <td>0.429</td>\n",
       "      <td>0.890</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1424</th>\n",
       "      <td>2021</td>\n",
       "      <td>Robert Covington</td>\n",
       "      <td>PF</td>\n",
       "      <td>30.0</td>\n",
       "      <td>POR</td>\n",
       "      <td>70.0</td>\n",
       "      <td>2243.0</td>\n",
       "      <td>11.2</td>\n",
       "      <td>0.553</td>\n",
       "      <td>0.699</td>\n",
       "      <td>...</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>0.080</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.379</td>\n",
       "      <td>0.451</td>\n",
       "      <td>0.806</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1425</th>\n",
       "      <td>2021</td>\n",
       "      <td>Damian Lillard</td>\n",
       "      <td>PG</td>\n",
       "      <td>30.0</td>\n",
       "      <td>POR</td>\n",
       "      <td>67.0</td>\n",
       "      <td>2398.0</td>\n",
       "      <td>25.6</td>\n",
       "      <td>0.623</td>\n",
       "      <td>0.528</td>\n",
       "      <td>...</td>\n",
       "      <td>9.6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>10.4</td>\n",
       "      <td>0.209</td>\n",
       "      <td>7.4</td>\n",
       "      <td>-1.6</td>\n",
       "      <td>0.391</td>\n",
       "      <td>0.519</td>\n",
       "      <td>0.928</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1426</th>\n",
       "      <td>2021</td>\n",
       "      <td>Will Barton</td>\n",
       "      <td>SF</td>\n",
       "      <td>30.0</td>\n",
       "      <td>DEN</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1736.0</td>\n",
       "      <td>11.8</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.423</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.061</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>-1.2</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.459</td>\n",
       "      <td>0.785</td>\n",
       "      <td>4.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1427</th>\n",
       "      <td>2021</td>\n",
       "      <td>JaMychal Green</td>\n",
       "      <td>PF</td>\n",
       "      <td>30.0</td>\n",
       "      <td>DEN</td>\n",
       "      <td>58.0</td>\n",
       "      <td>1120.0</td>\n",
       "      <td>13.6</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.529</td>\n",
       "      <td>...</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.110</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-1.6</td>\n",
       "      <td>0.399</td>\n",
       "      <td>0.534</td>\n",
       "      <td>0.807</td>\n",
       "      <td>4.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1428</th>\n",
       "      <td>2021</td>\n",
       "      <td>Paul Millsap</td>\n",
       "      <td>PF</td>\n",
       "      <td>35.0</td>\n",
       "      <td>DEN</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1162.0</td>\n",
       "      <td>16.4</td>\n",
       "      <td>0.565</td>\n",
       "      <td>0.357</td>\n",
       "      <td>...</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.6</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.343</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.724</td>\n",
       "      <td>4.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1429</th>\n",
       "      <td>2021</td>\n",
       "      <td>Nicolas Batum</td>\n",
       "      <td>SF</td>\n",
       "      <td>32.0</td>\n",
       "      <td>LAC</td>\n",
       "      <td>67.0</td>\n",
       "      <td>1835.0</td>\n",
       "      <td>12.9</td>\n",
       "      <td>0.617</td>\n",
       "      <td>0.660</td>\n",
       "      <td>...</td>\n",
       "      <td>2.7</td>\n",
       "      <td>2.3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.404</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.828</td>\n",
       "      <td>6.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1430</th>\n",
       "      <td>2021</td>\n",
       "      <td>Paul George</td>\n",
       "      <td>SF</td>\n",
       "      <td>30.0</td>\n",
       "      <td>LAC</td>\n",
       "      <td>54.0</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>20.5</td>\n",
       "      <td>0.598</td>\n",
       "      <td>0.437</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>5.3</td>\n",
       "      <td>0.139</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>0.411</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.868</td>\n",
       "      <td>6.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1431</th>\n",
       "      <td>2021</td>\n",
       "      <td>Reggie Jackson</td>\n",
       "      <td>SG</td>\n",
       "      <td>30.0</td>\n",
       "      <td>LAC</td>\n",
       "      <td>67.0</td>\n",
       "      <td>1544.0</td>\n",
       "      <td>14.2</td>\n",
       "      <td>0.576</td>\n",
       "      <td>0.482</td>\n",
       "      <td>...</td>\n",
       "      <td>2.3</td>\n",
       "      <td>1.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.433</td>\n",
       "      <td>0.465</td>\n",
       "      <td>0.817</td>\n",
       "      <td>6.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1432</th>\n",
       "      <td>2021</td>\n",
       "      <td>Marcus Morris</td>\n",
       "      <td>PF</td>\n",
       "      <td>31.0</td>\n",
       "      <td>LAC</td>\n",
       "      <td>57.0</td>\n",
       "      <td>1502.0</td>\n",
       "      <td>14.5</td>\n",
       "      <td>0.614</td>\n",
       "      <td>0.507</td>\n",
       "      <td>...</td>\n",
       "      <td>2.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.9</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.473</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.820</td>\n",
       "      <td>6.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1433</th>\n",
       "      <td>2021</td>\n",
       "      <td>Kent Bazemore</td>\n",
       "      <td>SF</td>\n",
       "      <td>31.0</td>\n",
       "      <td>GSW</td>\n",
       "      <td>67.0</td>\n",
       "      <td>1333.0</td>\n",
       "      <td>10.6</td>\n",
       "      <td>0.564</td>\n",
       "      <td>0.469</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>2.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.065</td>\n",
       "      <td>-2.9</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.486</td>\n",
       "      <td>0.692</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1434</th>\n",
       "      <td>2021</td>\n",
       "      <td>Stephen Curry</td>\n",
       "      <td>PG</td>\n",
       "      <td>32.0</td>\n",
       "      <td>GSW</td>\n",
       "      <td>63.0</td>\n",
       "      <td>2152.0</td>\n",
       "      <td>26.3</td>\n",
       "      <td>0.655</td>\n",
       "      <td>0.587</td>\n",
       "      <td>...</td>\n",
       "      <td>6.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.201</td>\n",
       "      <td>8.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.421</td>\n",
       "      <td>0.569</td>\n",
       "      <td>0.916</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1435</th>\n",
       "      <td>2021</td>\n",
       "      <td>Draymond Green</td>\n",
       "      <td>PF</td>\n",
       "      <td>30.0</td>\n",
       "      <td>GSW</td>\n",
       "      <td>63.0</td>\n",
       "      <td>1982.0</td>\n",
       "      <td>13.3</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.332</td>\n",
       "      <td>...</td>\n",
       "      <td>1.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>4.6</td>\n",
       "      <td>0.112</td>\n",
       "      <td>-1.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.535</td>\n",
       "      <td>0.795</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436</th>\n",
       "      <td>2021</td>\n",
       "      <td>Eric Bledsoe</td>\n",
       "      <td>SG</td>\n",
       "      <td>31.0</td>\n",
       "      <td>NOP</td>\n",
       "      <td>71.0</td>\n",
       "      <td>2111.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>0.533</td>\n",
       "      <td>0.486</td>\n",
       "      <td>...</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.3</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.053</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>-1.3</td>\n",
       "      <td>0.341</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.687</td>\n",
       "      <td>-0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>2021</td>\n",
       "      <td>Bojan Bogdanoviƒá</td>\n",
       "      <td>SF</td>\n",
       "      <td>31.0</td>\n",
       "      <td>UTA</td>\n",
       "      <td>72.0</td>\n",
       "      <td>2216.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.588</td>\n",
       "      <td>0.498</td>\n",
       "      <td>...</td>\n",
       "      <td>2.7</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>0.390</td>\n",
       "      <td>0.487</td>\n",
       "      <td>0.879</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1438</th>\n",
       "      <td>2021</td>\n",
       "      <td>Mike Conley</td>\n",
       "      <td>PG</td>\n",
       "      <td>33.0</td>\n",
       "      <td>UTA</td>\n",
       "      <td>51.0</td>\n",
       "      <td>1498.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>0.589</td>\n",
       "      <td>0.523</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>6.1</td>\n",
       "      <td>0.197</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.479</td>\n",
       "      <td>0.852</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439</th>\n",
       "      <td>2021</td>\n",
       "      <td>Joe Ingles</td>\n",
       "      <td>SF</td>\n",
       "      <td>33.0</td>\n",
       "      <td>UTA</td>\n",
       "      <td>67.0</td>\n",
       "      <td>1867.0</td>\n",
       "      <td>15.9</td>\n",
       "      <td>0.672</td>\n",
       "      <td>0.722</td>\n",
       "      <td>...</td>\n",
       "      <td>4.7</td>\n",
       "      <td>2.3</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.180</td>\n",
       "      <td>2.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.451</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.844</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1440</th>\n",
       "      <td>2021</td>\n",
       "      <td>Jimmy Butler</td>\n",
       "      <td>SF</td>\n",
       "      <td>31.0</td>\n",
       "      <td>MIA</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1745.0</td>\n",
       "      <td>26.5</td>\n",
       "      <td>0.607</td>\n",
       "      <td>0.139</td>\n",
       "      <td>...</td>\n",
       "      <td>6.6</td>\n",
       "      <td>2.7</td>\n",
       "      <td>9.3</td>\n",
       "      <td>0.255</td>\n",
       "      <td>5.1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.863</td>\n",
       "      <td>-0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1441</th>\n",
       "      <td>2021</td>\n",
       "      <td>Goran Dragiƒá</td>\n",
       "      <td>PG</td>\n",
       "      <td>34.0</td>\n",
       "      <td>MIA</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1337.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.552</td>\n",
       "      <td>0.440</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.048</td>\n",
       "      <td>-1.2</td>\n",
       "      <td>-1.8</td>\n",
       "      <td>0.373</td>\n",
       "      <td>0.479</td>\n",
       "      <td>0.828</td>\n",
       "      <td>-0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1442</th>\n",
       "      <td>2021</td>\n",
       "      <td>Andre Iguodala</td>\n",
       "      <td>SF</td>\n",
       "      <td>37.0</td>\n",
       "      <td>MIA</td>\n",
       "      <td>63.0</td>\n",
       "      <td>1339.0</td>\n",
       "      <td>9.2</td>\n",
       "      <td>0.519</td>\n",
       "      <td>0.734</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.062</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.658</td>\n",
       "      <td>-0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1443</th>\n",
       "      <td>2021</td>\n",
       "      <td>Seth Curry</td>\n",
       "      <td>SG</td>\n",
       "      <td>30.0</td>\n",
       "      <td>PHI</td>\n",
       "      <td>57.0</td>\n",
       "      <td>1638.0</td>\n",
       "      <td>12.9</td>\n",
       "      <td>0.607</td>\n",
       "      <td>0.507</td>\n",
       "      <td>...</td>\n",
       "      <td>2.3</td>\n",
       "      <td>1.7</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.118</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.896</td>\n",
       "      <td>5.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1444</th>\n",
       "      <td>2021</td>\n",
       "      <td>Danny Green</td>\n",
       "      <td>SF</td>\n",
       "      <td>33.0</td>\n",
       "      <td>PHI</td>\n",
       "      <td>69.0</td>\n",
       "      <td>1934.0</td>\n",
       "      <td>12.1</td>\n",
       "      <td>0.582</td>\n",
       "      <td>0.794</td>\n",
       "      <td>...</td>\n",
       "      <td>1.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>4.6</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.405</td>\n",
       "      <td>0.438</td>\n",
       "      <td>0.775</td>\n",
       "      <td>5.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1445</th>\n",
       "      <td>2021</td>\n",
       "      <td>Dwight Howard</td>\n",
       "      <td>C</td>\n",
       "      <td>35.0</td>\n",
       "      <td>PHI</td>\n",
       "      <td>69.0</td>\n",
       "      <td>1196.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>0.610</td>\n",
       "      <td>0.066</td>\n",
       "      <td>...</td>\n",
       "      <td>1.3</td>\n",
       "      <td>2.6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.159</td>\n",
       "      <td>-2.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.611</td>\n",
       "      <td>0.576</td>\n",
       "      <td>5.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1446</th>\n",
       "      <td>2021</td>\n",
       "      <td>DeMar DeRozan</td>\n",
       "      <td>PF</td>\n",
       "      <td>31.0</td>\n",
       "      <td>SAS</td>\n",
       "      <td>61.0</td>\n",
       "      <td>2056.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.591</td>\n",
       "      <td>0.080</td>\n",
       "      <td>...</td>\n",
       "      <td>5.9</td>\n",
       "      <td>1.5</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.172</td>\n",
       "      <td>3.4</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.257</td>\n",
       "      <td>0.515</td>\n",
       "      <td>0.880</td>\n",
       "      <td>-1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1447</th>\n",
       "      <td>2021</td>\n",
       "      <td>Rudy Gay</td>\n",
       "      <td>PF</td>\n",
       "      <td>34.0</td>\n",
       "      <td>SAS</td>\n",
       "      <td>63.0</td>\n",
       "      <td>1358.0</td>\n",
       "      <td>14.7</td>\n",
       "      <td>0.532</td>\n",
       "      <td>0.443</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.059</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.451</td>\n",
       "      <td>0.804</td>\n",
       "      <td>-1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1448</th>\n",
       "      <td>2021</td>\n",
       "      <td>Patty Mills</td>\n",
       "      <td>PG</td>\n",
       "      <td>32.0</td>\n",
       "      <td>SAS</td>\n",
       "      <td>68.0</td>\n",
       "      <td>1685.0</td>\n",
       "      <td>11.8</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.702</td>\n",
       "      <td>...</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-1.8</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.910</td>\n",
       "      <td>-1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1449</th>\n",
       "      <td>2021</td>\n",
       "      <td>Wayne Ellington</td>\n",
       "      <td>SG</td>\n",
       "      <td>33.0</td>\n",
       "      <td>DET</td>\n",
       "      <td>46.0</td>\n",
       "      <td>1012.0</td>\n",
       "      <td>12.2</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.814</td>\n",
       "      <td>...</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.088</td>\n",
       "      <td>1.3</td>\n",
       "      <td>-1.7</td>\n",
       "      <td>0.422</td>\n",
       "      <td>0.524</td>\n",
       "      <td>0.800</td>\n",
       "      <td>-4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1450</th>\n",
       "      <td>2021</td>\n",
       "      <td>Mason Plumlee</td>\n",
       "      <td>C</td>\n",
       "      <td>30.0</td>\n",
       "      <td>DET</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1499.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>0.638</td>\n",
       "      <td>0.018</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.626</td>\n",
       "      <td>0.669</td>\n",
       "      <td>-4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1451</th>\n",
       "      <td>2021</td>\n",
       "      <td>Danilo Gallinari</td>\n",
       "      <td>PF</td>\n",
       "      <td>32.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>51.0</td>\n",
       "      <td>1222.0</td>\n",
       "      <td>16.3</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.535</td>\n",
       "      <td>...</td>\n",
       "      <td>2.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.7</td>\n",
       "      <td>0.146</td>\n",
       "      <td>1.6</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>0.406</td>\n",
       "      <td>0.466</td>\n",
       "      <td>0.925</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1452</th>\n",
       "      <td>2021</td>\n",
       "      <td>Jrue Holiday</td>\n",
       "      <td>PG</td>\n",
       "      <td>30.0</td>\n",
       "      <td>MIL</td>\n",
       "      <td>59.0</td>\n",
       "      <td>1907.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.344</td>\n",
       "      <td>...</td>\n",
       "      <td>4.3</td>\n",
       "      <td>2.2</td>\n",
       "      <td>6.6</td>\n",
       "      <td>0.165</td>\n",
       "      <td>2.8</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.392</td>\n",
       "      <td>0.561</td>\n",
       "      <td>0.787</td>\n",
       "      <td>5.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1453</th>\n",
       "      <td>2021</td>\n",
       "      <td>Brook Lopez</td>\n",
       "      <td>C</td>\n",
       "      <td>32.0</td>\n",
       "      <td>MIL</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1902.0</td>\n",
       "      <td>15.4</td>\n",
       "      <td>0.611</td>\n",
       "      <td>0.439</td>\n",
       "      <td>...</td>\n",
       "      <td>3.1</td>\n",
       "      <td>2.2</td>\n",
       "      <td>5.3</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.338</td>\n",
       "      <td>0.632</td>\n",
       "      <td>0.845</td>\n",
       "      <td>5.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>2021</td>\n",
       "      <td>Justin Holiday</td>\n",
       "      <td>SG</td>\n",
       "      <td>31.0</td>\n",
       "      <td>IND</td>\n",
       "      <td>72.0</td>\n",
       "      <td>2183.0</td>\n",
       "      <td>10.4</td>\n",
       "      <td>0.571</td>\n",
       "      <td>0.725</td>\n",
       "      <td>...</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.8</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.069</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>0.382</td>\n",
       "      <td>0.494</td>\n",
       "      <td>0.788</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>2021</td>\n",
       "      <td>LeBron James</td>\n",
       "      <td>PG</td>\n",
       "      <td>36.0</td>\n",
       "      <td>LAL</td>\n",
       "      <td>45.0</td>\n",
       "      <td>1504.0</td>\n",
       "      <td>24.2</td>\n",
       "      <td>0.602</td>\n",
       "      <td>0.346</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>5.6</td>\n",
       "      <td>0.179</td>\n",
       "      <td>5.9</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.591</td>\n",
       "      <td>0.698</td>\n",
       "      <td>2.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>2021</td>\n",
       "      <td>Wesley Matthews</td>\n",
       "      <td>SG</td>\n",
       "      <td>34.0</td>\n",
       "      <td>LAL</td>\n",
       "      <td>58.0</td>\n",
       "      <td>1130.0</td>\n",
       "      <td>7.2</td>\n",
       "      <td>0.517</td>\n",
       "      <td>0.782</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.073</td>\n",
       "      <td>-3.1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.335</td>\n",
       "      <td>0.418</td>\n",
       "      <td>0.854</td>\n",
       "      <td>2.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>2021</td>\n",
       "      <td>Markieff Morris</td>\n",
       "      <td>PF</td>\n",
       "      <td>31.0</td>\n",
       "      <td>LAL</td>\n",
       "      <td>61.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>9.3</td>\n",
       "      <td>0.509</td>\n",
       "      <td>0.553</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.062</td>\n",
       "      <td>-2.8</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.311</td>\n",
       "      <td>0.521</td>\n",
       "      <td>0.720</td>\n",
       "      <td>2.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>2021</td>\n",
       "      <td>Robin Lopez</td>\n",
       "      <td>C</td>\n",
       "      <td>32.0</td>\n",
       "      <td>WAS</td>\n",
       "      <td>71.0</td>\n",
       "      <td>1354.0</td>\n",
       "      <td>16.7</td>\n",
       "      <td>0.661</td>\n",
       "      <td>0.042</td>\n",
       "      <td>...</td>\n",
       "      <td>2.8</td>\n",
       "      <td>0.9</td>\n",
       "      <td>3.7</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-1.4</td>\n",
       "      <td>0.278</td>\n",
       "      <td>0.649</td>\n",
       "      <td>0.723</td>\n",
       "      <td>-1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>2021</td>\n",
       "      <td>Russell Westbrook</td>\n",
       "      <td>PG</td>\n",
       "      <td>32.0</td>\n",
       "      <td>WAS</td>\n",
       "      <td>65.0</td>\n",
       "      <td>2369.0</td>\n",
       "      <td>19.5</td>\n",
       "      <td>0.509</td>\n",
       "      <td>0.221</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.2</td>\n",
       "      <td>3.7</td>\n",
       "      <td>0.075</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.315</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.656</td>\n",
       "      <td>-1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1460</th>\n",
       "      <td>2021</td>\n",
       "      <td>Kyle Lowry</td>\n",
       "      <td>PG</td>\n",
       "      <td>34.0</td>\n",
       "      <td>TOR</td>\n",
       "      <td>46.0</td>\n",
       "      <td>1601.0</td>\n",
       "      <td>16.5</td>\n",
       "      <td>0.593</td>\n",
       "      <td>0.555</td>\n",
       "      <td>...</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.3</td>\n",
       "      <td>4.1</td>\n",
       "      <td>0.124</td>\n",
       "      <td>1.9</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>0.396</td>\n",
       "      <td>0.487</td>\n",
       "      <td>0.875</td>\n",
       "      <td>-0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1461</th>\n",
       "      <td>2021</td>\n",
       "      <td>Ricky Rubio</td>\n",
       "      <td>PG</td>\n",
       "      <td>30.0</td>\n",
       "      <td>MIN</td>\n",
       "      <td>68.0</td>\n",
       "      <td>1772.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.430</td>\n",
       "      <td>...</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1.3</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.085</td>\n",
       "      <td>-2.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.447</td>\n",
       "      <td>0.867</td>\n",
       "      <td>-5.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1462</th>\n",
       "      <td>2021</td>\n",
       "      <td>Garrett Temple</td>\n",
       "      <td>SG</td>\n",
       "      <td>34.0</td>\n",
       "      <td>CHI</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1528.0</td>\n",
       "      <td>8.2</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.525</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.050</td>\n",
       "      <td>-3.3</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.335</td>\n",
       "      <td>0.503</td>\n",
       "      <td>0.800</td>\n",
       "      <td>-1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1463</th>\n",
       "      <td>2021</td>\n",
       "      <td>Thaddeus Young</td>\n",
       "      <td>PF</td>\n",
       "      <td>32.0</td>\n",
       "      <td>CHI</td>\n",
       "      <td>68.0</td>\n",
       "      <td>1652.0</td>\n",
       "      <td>20.3</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.068</td>\n",
       "      <td>...</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2.2</td>\n",
       "      <td>5.1</td>\n",
       "      <td>0.147</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.267</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.628</td>\n",
       "      <td>-1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1464</th>\n",
       "      <td>2021</td>\n",
       "      <td>Nikola Vuƒçeviƒá</td>\n",
       "      <td>C</td>\n",
       "      <td>30.0</td>\n",
       "      <td>ORL</td>\n",
       "      <td>44.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>23.5</td>\n",
       "      <td>0.565</td>\n",
       "      <td>0.317</td>\n",
       "      <td>...</td>\n",
       "      <td>2.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>3.9</td>\n",
       "      <td>0.125</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.406</td>\n",
       "      <td>0.515</td>\n",
       "      <td>0.827</td>\n",
       "      <td>-9.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1465</th>\n",
       "      <td>2021</td>\n",
       "      <td>Kemba Walker</td>\n",
       "      <td>PG</td>\n",
       "      <td>30.0</td>\n",
       "      <td>BOS</td>\n",
       "      <td>43.0</td>\n",
       "      <td>1369.0</td>\n",
       "      <td>17.7</td>\n",
       "      <td>0.559</td>\n",
       "      <td>0.522</td>\n",
       "      <td>...</td>\n",
       "      <td>2.1</td>\n",
       "      <td>1.1</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.115</td>\n",
       "      <td>2.5</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.486</td>\n",
       "      <td>0.899</td>\n",
       "      <td>1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466</th>\n",
       "      <td>2021</td>\n",
       "      <td>John Wall</td>\n",
       "      <td>PG</td>\n",
       "      <td>30.0</td>\n",
       "      <td>HOU</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1288.0</td>\n",
       "      <td>15.4</td>\n",
       "      <td>0.503</td>\n",
       "      <td>0.343</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>1.4</td>\n",
       "      <td>-1.6</td>\n",
       "      <td>0.317</td>\n",
       "      <td>0.449</td>\n",
       "      <td>0.749</td>\n",
       "      <td>-7.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Year             Player Pos   Age   Tm     G      MP   PER    TS%  \\\n",
       "1423  2021    Carmelo Anthony  PF  36.0  POR  69.0  1690.0  14.6  0.547   \n",
       "1424  2021   Robert Covington  PF  30.0  POR  70.0  2243.0  11.2  0.553   \n",
       "1425  2021     Damian Lillard  PG  30.0  POR  67.0  2398.0  25.6  0.623   \n",
       "1426  2021        Will Barton  SF  30.0  DEN  56.0  1736.0  11.8  0.538   \n",
       "1427  2021     JaMychal Green  PF  30.0  DEN  58.0  1120.0  13.6  0.590   \n",
       "1428  2021       Paul Millsap  PF  35.0  DEN  56.0  1162.0  16.4  0.565   \n",
       "1429  2021      Nicolas Batum  SF  32.0  LAC  67.0  1835.0  12.9  0.617   \n",
       "1430  2021        Paul George  SF  30.0  LAC  54.0  1821.0  20.5  0.598   \n",
       "1431  2021     Reggie Jackson  SG  30.0  LAC  67.0  1544.0  14.2  0.576   \n",
       "1432  2021      Marcus Morris  PF  31.0  LAC  57.0  1502.0  14.5  0.614   \n",
       "1433  2021      Kent Bazemore  SF  31.0  GSW  67.0  1333.0  10.6  0.564   \n",
       "1434  2021      Stephen Curry  PG  32.0  GSW  63.0  2152.0  26.3  0.655   \n",
       "1435  2021     Draymond Green  PF  30.0  GSW  63.0  1982.0  13.3  0.530   \n",
       "1436  2021       Eric Bledsoe  SG  31.0  NOP  71.0  2111.0  11.5  0.533   \n",
       "1437  2021  Bojan Bogdanoviƒá  SF  31.0  UTA  72.0  2216.0  14.0  0.588   \n",
       "1438  2021        Mike Conley  PG  33.0  UTA  51.0  1498.0  19.2  0.589   \n",
       "1439  2021         Joe Ingles  SF  33.0  UTA  67.0  1867.0  15.9  0.672   \n",
       "1440  2021       Jimmy Butler  SF  31.0  MIA  52.0  1745.0  26.5  0.607   \n",
       "1441  2021      Goran Dragiƒá  PG  34.0  MIA  50.0  1337.0  13.0  0.552   \n",
       "1442  2021     Andre Iguodala  SF  37.0  MIA  63.0  1339.0   9.2  0.519   \n",
       "1443  2021         Seth Curry  SG  30.0  PHI  57.0  1638.0  12.9  0.607   \n",
       "1444  2021        Danny Green  SF  33.0  PHI  69.0  1934.0  12.1  0.582   \n",
       "1445  2021      Dwight Howard   C  35.0  PHI  69.0  1196.0  17.8  0.610   \n",
       "1446  2021      DeMar DeRozan  PF  31.0  SAS  61.0  2056.0  22.0  0.591   \n",
       "1447  2021           Rudy Gay  PF  34.0  SAS  63.0  1358.0  14.7  0.532   \n",
       "1448  2021        Patty Mills  PG  32.0  SAS  68.0  1685.0  11.8  0.570   \n",
       "1449  2021    Wayne Ellington  SG  33.0  DET  46.0  1012.0  12.2  0.625   \n",
       "1450  2021      Mason Plumlee   C  30.0  DET  56.0  1499.0  18.7  0.638   \n",
       "1451  2021   Danilo Gallinari  PF  32.0  ATL  51.0  1222.0  16.3  0.613   \n",
       "1452  2021       Jrue Holiday  PG  30.0  MIL  59.0  1907.0  20.0  0.592   \n",
       "1453  2021        Brook Lopez   C  32.0  MIL  70.0  1902.0  15.4  0.611   \n",
       "1454  2021     Justin Holiday  SG  31.0  IND  72.0  2183.0  10.4  0.571   \n",
       "1455  2021       LeBron James  PG  36.0  LAL  45.0  1504.0  24.2  0.602   \n",
       "1456  2021    Wesley Matthews  SG  34.0  LAL  58.0  1130.0   7.2  0.517   \n",
       "1457  2021    Markieff Morris  PF  31.0  LAL  61.0  1200.0   9.3  0.509   \n",
       "1458  2021        Robin Lopez   C  32.0  WAS  71.0  1354.0  16.7  0.661   \n",
       "1459  2021  Russell Westbrook  PG  32.0  WAS  65.0  2369.0  19.5  0.509   \n",
       "1460  2021         Kyle Lowry  PG  34.0  TOR  46.0  1601.0  16.5  0.593   \n",
       "1461  2021        Ricky Rubio  PG  30.0  MIN  68.0  1772.0  13.5  0.516   \n",
       "1462  2021     Garrett Temple  SG  34.0  CHI  56.0  1528.0   8.2  0.525   \n",
       "1463  2021     Thaddeus Young  PF  32.0  CHI  68.0  1652.0  20.3  0.578   \n",
       "1464  2021   Nikola Vuƒçeviƒá   C  30.0  ORL  44.0  1500.0  23.5  0.565   \n",
       "1465  2021       Kemba Walker  PG  30.0  BOS  43.0  1369.0  17.7  0.559   \n",
       "1466  2021          John Wall  PG  30.0  HOU  40.0  1288.0  15.4  0.503   \n",
       "\n",
       "       3PAr  ...  OWS  DWS    WS  WS/48  OBPM  DBPM    3P%    2P%    FT%  \\\n",
       "1423  0.418  ...  1.7  0.8   2.6  0.073   0.1  -1.6  0.409  0.429  0.890   \n",
       "1424  0.699  ...  1.4  2.4   3.7  0.080  -1.5   1.2  0.379  0.451  0.806   \n",
       "1425  0.528  ...  9.6  0.8  10.4  0.209   7.4  -1.6  0.391  0.519  0.928   \n",
       "1426  0.423  ...  0.7  1.5   2.2  0.061  -0.8  -1.2  0.381  0.459  0.785   \n",
       "1427  0.529  ...  1.4  1.2   2.6  0.110  -0.4  -1.6  0.399  0.534  0.807   \n",
       "1428  0.357  ...  1.7  1.6   3.2  0.133   0.4   0.1  0.343  0.550  0.724   \n",
       "1429  0.660  ...  2.7  2.3   5.0  0.132   0.5   1.0  0.404  0.579  0.828   \n",
       "1430  0.437  ...  3.0  2.3   5.3  0.139   4.0  -0.4  0.411  0.510  0.868   \n",
       "1431  0.482  ...  2.3  1.4   3.7  0.115   0.5  -0.5  0.433  0.465  0.817   \n",
       "1432  0.507  ...  2.1  1.5   3.6  0.115   0.9  -1.0  0.473  0.472  0.820   \n",
       "1433  0.469  ... -0.2  2.1   1.8  0.065  -2.9   1.3  0.408  0.486  0.692   \n",
       "1434  0.587  ...  6.5  2.5   9.0  0.201   8.1   0.0  0.421  0.569  0.916   \n",
       "1435  0.332  ...  1.2  3.4   4.6  0.112  -1.7   3.3  0.270  0.535  0.795   \n",
       "1436  0.486  ...  1.1  1.3   2.3  0.053  -1.1  -1.3  0.341  0.496  0.687   \n",
       "1437  0.498  ...  2.7  2.5   5.2  0.113   0.1  -0.9  0.390  0.487  0.879   \n",
       "1438  0.523  ...  4.0  2.1   6.1  0.197   3.5   0.9  0.412  0.479  0.852   \n",
       "1439  0.722  ...  4.7  2.3   7.0  0.180   2.7   0.7  0.451  0.590  0.844   \n",
       "1440  0.139  ...  6.6  2.7   9.3  0.255   5.1   2.5  0.245  0.538  0.863   \n",
       "1441  0.440  ...  0.2  1.2   1.3  0.048  -1.2  -1.8  0.373  0.479  0.828   \n",
       "1442  0.734  ... -0.1  1.8   1.7  0.062  -2.5   1.8  0.330  0.530  0.658   \n",
       "1443  0.507  ...  2.3  1.7   4.0  0.118  -0.1  -0.8  0.450  0.485  0.896   \n",
       "1444  0.794  ...  1.6  3.1   4.6  0.115   0.2   1.2  0.405  0.438  0.775   \n",
       "1445  0.066  ...  1.3  2.6   4.0  0.159  -2.2   0.3  0.250  0.611  0.576   \n",
       "1446  0.080  ...  5.9  1.5   7.4  0.172   3.4  -0.2  0.257  0.515  0.880   \n",
       "1447  0.443  ...  0.0  1.7   1.7  0.059  -0.1  -0.2  0.381  0.451  0.804   \n",
       "1448  0.702  ...  1.3  0.8   2.2  0.062   0.1  -1.8  0.375  0.500  0.910   \n",
       "1449  0.814  ...  1.3  0.5   1.9  0.088   1.3  -1.7  0.422  0.524  0.800   \n",
       "1450  0.018  ...  3.0  2.1   5.1  0.164   0.3   1.8  0.000  0.626  0.669   \n",
       "1451  0.535  ...  2.7  1.0   3.7  0.146   1.6  -0.8  0.406  0.466  0.925   \n",
       "1452  0.344  ...  4.3  2.2   6.6  0.165   2.8   0.7  0.392  0.561  0.787   \n",
       "1453  0.439  ...  3.1  2.2   5.3  0.133   0.1  -0.1  0.338  0.632  0.845   \n",
       "1454  0.725  ...  1.4  1.8   3.1  0.069  -0.7  -0.4  0.382  0.494  0.788   \n",
       "1455  0.346  ...  3.0  2.6   5.6  0.179   5.9   1.6  0.365  0.591  0.698   \n",
       "1456  0.782  ...  0.2  1.5   1.7  0.073  -3.1   0.6  0.335  0.418  0.854   \n",
       "1457  0.553  ... -0.2  1.8   1.6  0.062  -2.8  -0.2  0.311  0.521  0.720   \n",
       "1458  0.042  ...  2.8  0.9   3.7  0.131   0.3  -1.4  0.278  0.649  0.723   \n",
       "1459  0.221  ...  0.5  3.2   3.7  0.075   2.5   0.8  0.315  0.475  0.656   \n",
       "1460  0.555  ...  2.9  1.3   4.1  0.124   1.9  -0.7  0.396  0.487  0.875   \n",
       "1461  0.430  ...  1.9  1.3   3.1  0.085  -2.1  -0.1  0.308  0.447  0.867   \n",
       "1462  0.525  ...  0.2  1.4   1.6  0.050  -3.3  -0.1  0.335  0.503  0.800   \n",
       "1463  0.068  ...  2.8  2.2   5.1  0.147   2.0   1.2  0.267  0.580  0.628   \n",
       "1464  0.317  ...  2.1  1.8   3.9  0.125   5.5   0.1  0.406  0.515  0.827   \n",
       "1465  0.522  ...  2.1  1.1   3.3  0.115   2.5  -0.7  0.360  0.486  0.899   \n",
       "1466  0.343  ... -1.0  0.8  -0.2 -0.007   1.4  -1.6  0.317  0.449  0.749   \n",
       "\n",
       "      TmNetRtg  \n",
       "1423       1.8  \n",
       "1424       1.8  \n",
       "1425       1.8  \n",
       "1426       4.8  \n",
       "1427       4.8  \n",
       "1428       4.8  \n",
       "1429       6.1  \n",
       "1430       6.1  \n",
       "1431       6.1  \n",
       "1432       6.1  \n",
       "1433       1.1  \n",
       "1434       1.1  \n",
       "1435       1.1  \n",
       "1436      -0.3  \n",
       "1437       9.0  \n",
       "1438       9.0  \n",
       "1439       9.0  \n",
       "1440      -0.1  \n",
       "1441      -0.1  \n",
       "1442      -0.1  \n",
       "1443       5.5  \n",
       "1444       5.5  \n",
       "1445       5.5  \n",
       "1446      -1.5  \n",
       "1447      -1.5  \n",
       "1448      -1.5  \n",
       "1449      -4.5  \n",
       "1450      -4.5  \n",
       "1451       2.2  \n",
       "1452       5.8  \n",
       "1453       5.8  \n",
       "1454       0.1  \n",
       "1455       2.9  \n",
       "1456       2.9  \n",
       "1457       2.9  \n",
       "1458      -1.6  \n",
       "1459      -1.6  \n",
       "1460      -0.4  \n",
       "1461      -5.3  \n",
       "1462      -1.1  \n",
       "1463      -1.1  \n",
       "1464      -9.3  \n",
       "1465       1.2  \n",
       "1466      -7.4  \n",
       "\n",
       "[44 rows x 28 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "starter = data2.loc[(data2['WS/48']>=0.1) & (data2['MP']/data2['G']>25),'Player']\n",
    "all_star = data2.loc[(data2['WS/48']>=0.15) & (data2['MP']/data2['G']>30) & (data2['USG%'] > 20),'Player']\n",
    "super_star = data2.loc[(data2['WS/48']>=0.15) & (data2['MP']/data2['G']>30) & (data2['USG%'] > 30),'Player']\n",
    "player_level = []\n",
    "for p in data2['Player']:\n",
    "    if p in list(super_star):\n",
    "        player_level.append(3)\n",
    "    elif p in list(all_star) and p not in list(super_star):\n",
    "        player_level.append(2)\n",
    "    elif p in list(starter) and p not in list(super_star) and p not in list(all_star):\n",
    "        player_level.append(1)\n",
    "    else:\n",
    "        player_level.append(0)\n",
    "data2['Player Level'] = player_level\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2_copy = data2.copy()\n",
    "data2.drop(['Age','Year','Player','Pos','Tm','WS','TmNetRtg'], axis=1, inplace=True)\n",
    "len(data2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "vv21 = vot_soft.predict(scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, [('Damian Lillard', 0), ('Stephen Curry', 2), ('LeBron James', 2)]) \n",
      "\n",
      "(0, [('Carmelo Anthony', 2), ('Robert Covington', 2), ('Will Barton', 4), ('JaMychal Green', 3), ('Paul Millsap', 4), ('Reggie Jackson', 2), ('Kent Bazemore', 2), ('Eric Bledsoe', 2), ('Goran Dragiƒá', 4), ('Andre Iguodala', 2), ('Dwight Howard', 2), ('Rudy Gay', 4), ('Patty Mills', 2), ('Wayne Ellington', 4), ('Danilo Gallinari', 4), ('Justin Holiday', 2), ('Wesley Matthews', 2), ('Markieff Morris', 3), ('Robin Lopez', 2), ('Russell Westbrook', 2), ('Ricky Rubio', 2), ('Garrett Temple', 2), ('Thaddeus Young', 2), ('John Wall', 4)]) \n",
      "\n",
      "(1, [('Nicolas Batum', 2), ('Paul George', 2), ('Marcus Morris', 2), ('Draymond Green', 2), ('Bojan Bogdanoviƒá', 2), ('Mike Conley', 2), ('Joe Ingles', 2), ('Seth Curry', 2), ('Danny Green', 2), ('Mason Plumlee', 2), ('Brook Lopez', 2), ('Kyle Lowry', 4), ('Nikola Vuƒçeviƒá', 4), ('Kemba Walker', 4)]) \n",
      "\n",
      "(2, [('Jimmy Butler', 2), ('DeMar DeRozan', 0), ('Jrue Holiday', 2)]) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nfor k,v in preds.items():\\n    print(k,\":\",v)\\n    print(\\'\\n\\')\\n'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "preds = defaultdict(list)\n",
    "for t in zip(data2_copy['Player'], vv21,data2_copy['Player Level']):\n",
    "    preds[t[2]].append(tuple((t[0], t[1])))\n",
    "preds = sorted(preds.items(),key=lambda x: (x[1][1],x[0]),reverse=True)\n",
    "for i in preds:\n",
    "    print(i,'\\n')\n",
    "\n",
    "'''\n",
    "for k,v in preds.items():\n",
    "    print(k,\":\",v)\n",
    "    print('\\n')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
